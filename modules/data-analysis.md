---
layout: page
title: æ•°æ®åˆ†æ
---

# æ•°æ®åˆ†æ

> ğŸ¯ **æ¨¡å—ç›®æ ‡**ï¼šæ·±å…¥æŒæ¡æ•°æ®åˆ†ææ–¹æ³•ï¼ŒæŒ–æ˜æ•°æ®ä»·å€¼å’Œæ´å¯Ÿ

## ğŸ“Š åˆ†ææ¦‚è¿°

æ•°æ®åˆ†ææ˜¯ä»åŸå§‹æ•°æ®ä¸­æå–æœ‰ä»·å€¼ä¿¡æ¯å’Œæ´å¯Ÿçš„ç³»ç»Ÿæ€§è¿‡ç¨‹ã€‚æœ¬æ¨¡å—å°†ä»‹ç»ç°ä»£æ•°æ®åˆ†æçš„æ ¸å¿ƒæ–¹æ³•ã€å·¥å…·å’Œæœ€ä½³å®è·µã€‚

## ğŸ” åˆ†ææ–¹æ³•è®º

### æ•°æ®åˆ†ææµç¨‹
1. **é—®é¢˜å®šä¹‰**ï¼šæ˜ç¡®åˆ†æç›®æ ‡å’Œä¸šåŠ¡éœ€æ±‚
2. **æ•°æ®æ”¶é›†**ï¼šè·å–ç›¸å…³æ•°æ®æº
3. **æ•°æ®æ¢ç´¢**ï¼šåˆæ­¥äº†è§£æ•°æ®ç‰¹å¾
4. **æ•°æ®æ¸…æ´—**ï¼šå¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
5. **ç‰¹å¾å·¥ç¨‹**ï¼šåˆ›å»ºå’Œé€‰æ‹©å…³é”®ç‰¹å¾
6. **æ¨¡å‹æ„å»º**ï¼šé€‰æ‹©åˆé€‚çš„åˆ†ææ–¹æ³•
7. **ç»“æœéªŒè¯**ï¼šè¯„ä¼°åˆ†æç»“æœçš„å¯é æ€§
8. **æ´å¯Ÿæå–**ï¼šæ€»ç»“å…³é”®å‘ç°å’Œå»ºè®®

### åˆ†æç±»å‹
- **æè¿°æ€§åˆ†æ**ï¼šäº†è§£"å‘ç”Ÿäº†ä»€ä¹ˆ"
- **è¯Šæ–­æ€§åˆ†æ**ï¼šè§£é‡Š"ä¸ºä»€ä¹ˆå‘ç”Ÿ"
- **é¢„æµ‹æ€§åˆ†æ**ï¼šé¢„æµ‹"å°†è¦å‘ç”Ÿä»€ä¹ˆ"
- **æŒ‡å¯¼æ€§åˆ†æ**ï¼šå»ºè®®"åº”è¯¥åšä»€ä¹ˆ"

## ğŸ“ˆ ç»Ÿè®¡åˆ†æåŸºç¡€

### æè¿°æ€§ç»Ÿè®¡
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# åŠ è½½æ•°æ®
data = pd.read_csv('dataset.csv')

# åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print(data.describe())

# æ•°æ®åˆ†å¸ƒ
plt.figure(figsize=(12, 8))
plt.subplot(2, 2, 1)
plt.hist(data['column1'], bins=30)
plt.title('æ•°æ®åˆ†å¸ƒ')

plt.subplot(2, 2, 2)
plt.boxplot(data['column2'])
plt.title('ç®±çº¿å›¾')

plt.subplot(2, 2, 3)
sns.scatterplot(x='x', y='y', data=data)
plt.title('æ•£ç‚¹å›¾')

plt.subplot(2, 2, 4)
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True)
plt.title('ç›¸å…³æ€§çŸ©é˜µ')

plt.tight_layout()
plt.show()
```

### å‡è®¾æ£€éªŒ
```python
from scipy import stats

# tæ£€éªŒ
group1 = data[data['group'] == 'A']['value']
group2 = data[data['group'] == 'B']['value']

t_stat, p_value = stats.ttest_ind(group1, group2)
print(f'Tç»Ÿè®¡é‡: {t_stat:.4f}')
print(f'På€¼: {p_value:.4f}')

# å¡æ–¹æ£€éªŒ
contingency_table = pd.crosstab(data['category1'], data['category2'])
chi2, p_val, dof, expected = stats.chi2_contingency(contingency_table)
print(f'å¡æ–¹ç»Ÿè®¡é‡: {chi2:.4f}')
print(f'På€¼: {p_val:.4f}')
```

## ğŸ¤– æœºå™¨å­¦ä¹ åˆ†æ

### ç›‘ç£å­¦ä¹ 
```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# æ•°æ®å‡†å¤‡
X = data.drop(['target'], axis=1)
y = data['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æ¨¡å‹è®­ç»ƒ
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# é¢„æµ‹å’Œè¯„ä¼°
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='importance', y='feature', data=feature_importance.head(10))
plt.title('ç‰¹å¾é‡è¦æ€§æ’å')
plt.show()
```

### æ— ç›‘ç£å­¦ä¹ 
```python
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-meansèšç±»
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# PCAé™ç»´å¯è§†åŒ–
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')
plt.colorbar(scatter)
plt.title('èšç±»ç»“æœå¯è§†åŒ–')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
plt.show()
```

## ğŸ“Š æ—¶é—´åºåˆ—åˆ†æ

### è¶‹åŠ¿åˆ†æ
```python
import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA

# æ—¶é—´åºåˆ—æ•°æ®å‡†å¤‡
ts_data = pd.read_csv('timeseries.csv', parse_dates=['date'], index_col='date')

# å­£èŠ‚æ€§åˆ†è§£
decomposition = seasonal_decompose(ts_data['value'], model='additive')

fig, axes = plt.subplots(4, 1, figsize=(12, 10))
decomposition.observed.plot(ax=axes[0], title='åŸå§‹æ•°æ®')
decomposition.trend.plot(ax=axes[1], title='è¶‹åŠ¿')
decomposition.seasonal.plot(ax=axes[2], title='å­£èŠ‚æ€§')
decomposition.resid.plot(ax=axes[3], title='æ®‹å·®')
plt.tight_layout()
plt.show()
```

### é¢„æµ‹å»ºæ¨¡
```python
# ARIMAæ¨¡å‹
model = ARIMA(ts_data['value'], order=(1, 1, 1))
fitted_model = model.fit()

# é¢„æµ‹
forecast = fitted_model.forecast(steps=30)
confidence_intervals = fitted_model.get_forecast(steps=30).conf_int()

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
plt.figure(figsize=(12, 6))
plt.plot(ts_data.index, ts_data['value'], label='å†å²æ•°æ®')
plt.plot(forecast.index, forecast, label='é¢„æµ‹', color='red')
plt.fill_between(confidence_intervals.index, 
                confidence_intervals.iloc[:, 0], 
                confidence_intervals.iloc[:, 1], 
                alpha=0.3, color='red', label='ç½®ä¿¡åŒºé—´')
plt.legend()
plt.title('æ—¶é—´åºåˆ—é¢„æµ‹')
plt.show()
```

## ğŸ” é«˜çº§åˆ†ææŠ€æœ¯

### A/Bæµ‹è¯•åˆ†æ
```python
import scipy.stats as stats

# A/Bæµ‹è¯•æ•°æ®
group_a = data[data['variant'] == 'A']['conversion_rate']
group_b = data[data['variant'] == 'B']['conversion_rate']

# ç»Ÿè®¡æ£€éªŒ
t_stat, p_value = stats.ttest_ind(group_a, group_b)
effect_size = (group_b.mean() - group_a.mean()) / np.sqrt(
    ((group_a.var() + group_b.var()) / 2)
)

print(f'Aç»„è½¬åŒ–ç‡: {group_a.mean():.4f}')
print(f'Bç»„è½¬åŒ–ç‡: {group_b.mean():.4f}')
print(f'æ•ˆåº”å¤§å°: {effect_size:.4f}')
print(f'På€¼: {p_value:.4f}')

# ç»“æœå¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.hist(group_a, alpha=0.7, label='Aç»„', bins=30)
plt.hist(group_b, alpha=0.7, label='Bç»„', bins=30)
plt.legend()
plt.title('A/Bæµ‹è¯•ç»“æœåˆ†å¸ƒ')
plt.xlabel('è½¬åŒ–ç‡')
plt.ylabel('é¢‘æ¬¡')
plt.show()
```

### ç”Ÿå­˜åˆ†æ
```python
from lifelines import KaplanMeierFitter
from lifelines.statistics import logrank_test

# ç”Ÿå­˜åˆ†ææ•°æ®
kmf = KaplanMeierFitter()

# æŒ‰ç»„åˆ†æ
groups = data['group'].unique()
plt.figure(figsize=(10, 6))

for group in groups:
    group_data = data[data['group'] == group]
    kmf.fit(group_data['duration'], group_data['event'])
    plt.plot(kmf.timeline, kmf.survival_function_, label=f'ç»„ {group}')

plt.xlabel('æ—¶é—´')
plt.ylabel('ç”Ÿå­˜æ¦‚ç‡')
plt.title('ç”Ÿå­˜æ›²çº¿')
plt.legend()
plt.show()

# å¯¹æ•°ç§©æ£€éªŒ
group1_data = data[data['group'] == 'A']
group2_data = data[data['group'] == 'B']

results = logrank_test(
    group1_data['duration'], group2_data['duration'],
    group1_data['event'], group2_data['event']
)
print(f'å¯¹æ•°ç§©æ£€éªŒ På€¼: {results.p_value:.4f}')
```

## ğŸ“‹ åˆ†ææŠ¥å‘Š

### è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ
```python
import matplotlib.pyplot as plt
from fpdf import FPDF
import seaborn as sns

class AnalysisReport:
    def __init__(self, data):
        self.data = data
        self.pdf = FPDF()
    
    def generate_summary(self):
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        summary = {
            'total_records': len(self.data),
            'features': list(self.data.columns),
            'missing_values': self.data.isnull().sum().to_dict(),
            'basic_stats': self.data.describe().to_dict()
        }
        return summary
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        # ç›¸å…³æ€§çƒ­å›¾
        plt.figure(figsize=(10, 8))
        sns.heatmap(self.data.corr(), annot=True, cmap='coolwarm')
        plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
        plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # åˆ†å¸ƒå›¾
        numeric_columns = self.data.select_dtypes(include=[np.number]).columns
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        for i, col in enumerate(numeric_columns[:4]):
            row, col_idx = i // 2, i % 2
            axes[row, col_idx].hist(self.data[col], bins=30)
            axes[row, col_idx].set_title(f'{col} åˆ†å¸ƒ')
        plt.tight_layout()
        plt.savefig('distributions.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_insights(self):
        """ç”Ÿæˆæ´å¯Ÿå’Œå»ºè®®"""
        insights = []
        
        # æ•°æ®è´¨é‡æ´å¯Ÿ
        missing_ratio = self.data.isnull().sum() / len(self.data)
        high_missing = missing_ratio[missing_ratio > 0.1]
        if not high_missing.empty:
            insights.append(f"å‘ç° {len(high_missing)} ä¸ªç‰¹å¾ç¼ºå¤±å€¼è¶…è¿‡10%")
        
        # ç›¸å…³æ€§æ´å¯Ÿ
        corr_matrix = self.data.corr()
        high_corr = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                if abs(corr_matrix.iloc[i, j]) > 0.8:
                    high_corr.append((corr_matrix.columns[i], 
                                   corr_matrix.columns[j], 
                                   corr_matrix.iloc[i, j]))
        
        if high_corr:
            insights.append(f"å‘ç° {len(high_corr)} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾")
        
        return insights

# ä½¿ç”¨ç¤ºä¾‹
report = AnalysisReport(data)
summary = report.generate_summary()
report.create_visualizations()
insights = report.generate_insights()

print("åˆ†ææ‘˜è¦:")
for key, value in summary.items():
    print(f"{key}: {value}")

print("\nå…³é”®æ´å¯Ÿ:")
for insight in insights:
    print(f"- {insight}")
```

## ğŸ“Š äº¤äº’å¼åˆ†æ

### Jupyter Dashboard
```python
import ipywidgets as widgets
from IPython.display import display
import plotly.express as px

# äº¤äº’å¼æ•°æ®æ¢ç´¢
def interactive_analysis():
    # åˆ›å»ºæ§ä»¶
    column_selector = widgets.Dropdown(
        options=data.columns.tolist(),
        description='é€‰æ‹©åˆ—:'
    )
    
    chart_type = widgets.RadioButtons(
        options=['ç›´æ–¹å›¾', 'æ•£ç‚¹å›¾', 'ç®±çº¿å›¾'],
        description='å›¾è¡¨ç±»å‹:'
    )
    
    # è¾“å‡ºåŒºåŸŸ
    output = widgets.Output()
    
    def update_chart(change):
        with output:
            output.clear_output()
            
            selected_column = column_selector.value
            chart_style = chart_type.value
            
            if chart_style == 'ç›´æ–¹å›¾':
                fig = px.histogram(data, x=selected_column)
            elif chart_style == 'æ•£ç‚¹å›¾':
                if len(data.columns) > 1:
                    other_col = [col for col in data.columns if col != selected_column][0]
                    fig = px.scatter(data, x=selected_column, y=other_col)
                else:
                    fig = px.histogram(data, x=selected_column)
            else:  # ç®±çº¿å›¾
                fig = px.box(data, y=selected_column)
            
            fig.show()
    
    # ç»‘å®šäº‹ä»¶
    column_selector.observe(update_chart, names='value')
    chart_type.observe(update_chart, names='value')
    
    # æ˜¾ç¤ºç•Œé¢
    display(widgets.VBox([column_selector, chart_type, output]))
    
    # åˆå§‹æ˜¾ç¤º
    update_chart(None)

# è°ƒç”¨äº¤äº’å¼åˆ†æ
interactive_analysis()
```

## ğŸ”— ç›¸å…³èµ„æº

### åˆ†æå·¥å…·
- **Pythonç”Ÿæ€ç³»ç»Ÿ**ï¼šPandas, NumPy, Scikit-learn
- **Rè¯­è¨€**ï¼šç»Ÿè®¡åˆ†æä¸“ç”¨è¯­è¨€
- **SQL**ï¼šæ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æ
- **Tableau/Power BI**ï¼šå•†ä¸šæ™ºèƒ½å·¥å…·

### åœ¨çº¿èµ„æº
- [Kaggle Learn](https://www.kaggle.com/learn)
- [Courseraæ•°æ®ç§‘å­¦è¯¾ç¨‹](https://www.coursera.org/browse/data-science)
- [edXç»Ÿè®¡å­¦ä¹ ](https://www.edx.org/course/statistical-learning)

## ğŸ”— å¯¼èˆªé“¾æ¥

- [è¿”å›ä¸»é¡µ](../index.html)
- [ä¸Šä¸€æ¨¡å—ï¼šPOINTSå·¥å…·ä»‹ç»](points-tool.html)
- [ä¸‹ä¸€æ¨¡å—ï¼šæ•°æ®æå–ä¸åˆ†æ](data-extraction.html)
