---
layout: page
title: æ•°æ®æ ‡æ³¨
---

# æ•°æ®æ ‡æ³¨

> ğŸ·ï¸ **æ¨¡å—ç›®æ ‡**ï¼šæŒæ¡æ•°æ®æ ‡æ³¨æŠ€æœ¯ï¼Œæ„å»ºé«˜è´¨é‡è®­ç»ƒæ•°æ®é›†

## ğŸ“– æ•°æ®æ ‡æ³¨æ¦‚è¿°

æ•°æ®æ ‡æ³¨æ˜¯æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¡¹ç›®çš„åŸºç¡€å·¥ä½œï¼Œæ¶‰åŠä¸ºåŸå§‹æ•°æ®æ·»åŠ æ ‡ç­¾ã€æ³¨é‡Šæˆ–å…ƒæ•°æ®ï¼Œä»¥ä¾¿ç®—æ³•èƒ½å¤Ÿå­¦ä¹ å’Œè¯†åˆ«æ¨¡å¼ã€‚é«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®æ˜¯æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚

## ğŸ’¡æ•°æ®æ ‡æ³¨åŸç†

æ•°æ®æ ‡æ³¨æ˜¯æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œå®ƒæŒ‡çš„æ˜¯ç»™åŸå§‹æ•°æ®ï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰ï¼‰æ·»åŠ æ ‡ç­¾æˆ–æ³¨é‡Šï¼Œä»¥ä¾¿è®­ç»ƒç›‘ç£å­¦ä¹ æ¨¡å‹ã€‚æ ‡æ³¨çš„ç›®çš„æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿç†è§£æ•°æ®çš„å«ä¹‰ï¼Œä»è€Œè¿›è¡Œå‡†ç¡®çš„é¢„æµ‹å’Œåˆ†ç±»ã€‚åœ¨â€œæ— ç›‘ç£â€æˆ–â€œè‡ªç›‘ç£â€è¯­å¢ƒä¸‹ï¼Œâ€œæ•°æ®æ ‡æ³¨â€ä¸å†ä¾èµ–äººå·¥ï¼Œè€Œæ˜¯è®©ç®—æ³•è‡ªå·±ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆpseudo labelï¼‰ã€‚

###  æœªç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰

- **å®šä¹‰**ï¼šæœªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå…¶ä¸­æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªä½¿ç”¨æœªæ ‡æ³¨çš„æ•°æ®ï¼Œæ¨¡å‹éœ€è¦è‡ªå·±å‘ç°æ•°æ®ä¸­çš„ç»“æ„ã€æ¨¡å¼æˆ–åˆ†å¸ƒï¼Œè€Œæ— éœ€ä»»ä½•å¤–éƒ¨æ ‡ç­¾çš„æŒ‡å¯¼ã€‚

- **ç‰¹ç‚¹**ï¼š
  
  - **æ— éœ€æ ‡æ³¨æ•°æ®**ï¼šæ¨¡å‹è®­ç»ƒä¸ä¾èµ–äºäººå·¥æ ‡æ³¨ï¼Œé™ä½äº†æ•°æ®å‡†å¤‡çš„æˆæœ¬å’Œæ—¶é—´ã€‚
  - **å‘ç°éšè—æ¨¡å¼**ï¼šèƒ½å¤ŸæŒ–æ˜æ•°æ®ä¸­æ½œåœ¨çš„ç»“æ„å’Œå…³ç³»ï¼Œä¾‹å¦‚é€šè¿‡èšç±»å°†ç›¸ä¼¼çš„æ•°æ®ç‚¹åˆ†ç»„ã€‚
  - **è¯„ä¼°å›°éš¾**: ç¼ºä¹ground-truth,éœ€è¦é—´æ¥æŒ‡æ ‡(ä¾‹å¦‚å¯è§†äººå·¥æ£€æŸ¥ã€ä¸‹æ¸¸ä»»åŠ¡æ•ˆæœ)è¡¡é‡
  
- **ä¸»è¦ä»»åŠ¡**ï¼š
  
  - **èšç±»**ï¼šèšç±»æ˜¯ä¸€ç§æ•°æ®æŒ–æ˜æŠ€æœ¯ï¼Œå®ƒæ ¹æ®æœªæ ‡è®°æ•°æ®çš„ç›¸ä¼¼æ€§æˆ–å·®å¼‚å¯¹å…¶è¿›è¡Œåˆ†ç»„ã€‚èšç±»ç®—æ³•ç”¨äºå°†åŸå§‹çš„ã€æœªåˆ†ç±»çš„æ•°æ®å¯¹è±¡å¤„ç†æˆç”±ä¿¡æ¯ä¸­çš„ç»“æ„æˆ–æ¨¡å¼è¡¨ç¤ºçš„ç»„ã€‚
  
  ![image-20251031151116261](.\data-annotation-images\image-20251031151116261.png)
  
  - **åº”ç”¨**ï¼š
   1. é©¾é©¶é£æ ¼ç”»åƒï¼Œä¸ºä¸ªæ€§åŒ–è‡ªé€‚åº”å·¡èˆªï¼ˆACCï¼‰æä¾›å‚æ•°æ¨¡æ¿
   2. è½¨è¿¹æ¨¡å¼æŒ–æ˜ï¼Œå¯¹ç™¾ä¸‡æ¡è‡ªç„¶é©¾é©¶è½¨è¿¹èšç±»ï¼Œç”¨äºè§„åˆ’æ¨¡å—å‚è€ƒè·¯å¾„åº“
   3. åŸå¸‚è·¯ç½‘æŠŠ GPS è½¨è¿¹èšç±»ï¼Œå‘ç°å¸¸å‘æ‹¥å µçš„â€œçƒ­ç‚¹è·¯æ®µâ€
   4. åŸºå› è¡¨è¾¾è°±èšç±»ï¼Œæ‰¾å‡ºæœªçŸ¥äºšå‹ç™Œç—‡

  - **å…³è”è§„åˆ™**ï¼šå…³è”è§„åˆ™æ˜¯ä¸€ç§åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œç”¨äºæŸ¥æ‰¾ç»™å®šæ•°æ®é›†ä¸­çš„å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚ä¾‹å¦‚è¶…å¸‚è´­ç‰©ç¯®åˆ†ææ˜¯å…³è”è§„åˆ™åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­çš„ç»å…¸åº”ç”¨ï¼Œå®ƒè‡ªåŠ¨ä»äº¤æ˜“æ•°æ®ä¸­å‘ç°å•†å“ä¹‹é—´çš„å…±ä¹°å…³ç³»ï¼Œæ— éœ€ä»»ä½•æ ‡ç­¾æˆ–äººå·¥å¹²é¢„ã€‚
  
  - ![image-20251031154954642](.\data-annotation-images\image-20251031154954642.png)
  
  - **åº”ç”¨**ï¼š
   1. è¶…å¸‚â€œå•¤é…’â†’å°¿å¸ƒâ€ç»å…¸è´§æ¶æ‘†æ”¾
   2. è§†é¢‘å¹³å°â€œçœ‹å®Œã€Šç‹‚é£™ã€‹â†’æ¥ç€çœ‹ã€Šæ²‰é»˜çš„çœŸç›¸ã€‹â€æ¨è
   3. åŒ»é™¢ç”µå­ç—…å†ï¼šå‡ºç°â€œå’³å—½+å‘çƒ­â€å¸¸ä¼´éšâ€œCT ç™½è‰²ç£¨ç»ç’ƒå½±â€ï¼Œè¾…åŠ©è¯Šæ–­
   4. é“¶è¡Œåæ´—é’±ï¼šåŒä¸€ IP çŸ­æ—¶é—´å†…â€œå¤šç¬”å°é¢è½¬å…¥â†’ç«‹åˆ»å¤§é¢è½¬å‡ºâ€è§¦å‘å‘Šè­¦

  - **é™ç»´**ï¼šè™½ç„¶æ›´å¤šæ•°æ®é€šå¸¸ä¼šäº§ç”Ÿæ›´å‡†ç¡®çš„ç»“æœï¼Œä½†å®ƒä¹Ÿä¼šå½±å“æœºå™¨å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼ˆä¾‹å¦‚è¿‡æ‹Ÿåˆï¼‰ï¼Œè¿˜å¯èƒ½ä½¿æ•°æ®é›†éš¾ä»¥å¯è§†åŒ–ã€‚å½“ç»™å®šçš„æ•°æ®é›†ä¸­çš„ç‰¹å¾æˆ–ç»´åº¦æ•°é‡è¿‡é«˜æ—¶ï¼Œå¯ä»¥ä½¿ç”¨é™ç»´æŠ€æœ¯ã€‚å®ƒå°†æ•°æ®è¾“å…¥çš„æ•°é‡å‡å°‘åˆ°åˆé€‚çš„å¤§å°ï¼ŒåŒæ—¶å°½å¯èƒ½åœ°ä¿æŒæ•°æ®é›†çš„å®Œæ•´æ€§ã€‚ä¾‹å¦‚ï¼ŒAutoEncoderé™ç»´
  
    ![image-20251031164408402](.\data-annotation-images\image-20251031164408402.png)
  
  - **åº”ç”¨**ï¼š
   1. äººè„¸è¯†åˆ«å…ˆæŠŠ 512-D ç‰¹å¾é™åˆ° 2-Dï¼Œå¯è§†åŒ–æŸ¥çœ‹ä¸åŒæ—ç¾¤åˆ†å¸ƒ
   2. å·¥ä¸šè´¨æ£€é«˜å…‰è°±å›¾åƒ 200 ä¸ªæ³¢æ®µâ†’3 ä¸ªä¸»æˆåˆ†ï¼Œå‹ç¼© 98 % å­˜å‚¨
   3. æ–‡æœ¬èˆ†æƒ…æŠŠ 10 ä¸‡è¯è¢‹ç»´æ•°é™åˆ° 50-D LDA ä¸»é¢˜ï¼Œå¿«é€Ÿæ£€ç´¢
   4. é‡‘èé£æ§ 1000+ å˜é‡â†’10 ç»´å› å­ï¼Œå–‚ç»™é€»è¾‘å›å½’é˜²æ­¢è¿‡æ‹Ÿåˆ
  
  - **å¼‚å¸¸æ£€æµ‹ï¼š**åœ¨åªæœ‰â€œæ­£å¸¸â€æ ·æœ¬ï¼ˆæˆ–æå°‘é‡å¼‚å¸¸ï¼‰çš„å‰æä¸‹ï¼Œè®©ç®—æ³•è‡ªå·±å­¦ä¼šâ€œä»€ä¹ˆæ˜¯æ­£å¸¸â€ï¼Œä»è€ŒæŠŠåç¦»æ­£å¸¸åˆ†å¸ƒçš„è§‚æµ‹åˆ¤ä¸ºå¼‚å¸¸ã€‚
  
  - ![image-20251031164755623](.\data-annotation-images\image-20251031164755623.png)

  - **åº”ç”¨**ï¼š
   1. è¾¹ç¼˜åœºæ™¯å‘ç°ï¼Œåœ¨å¤§è§„æ¨¡é©¾é©¶æ•°æ®ä¸­æ‰¾å‡ºè¾¹ç¼˜åœºæ™¯
   2. ä¿¡è¯„ç³»ç»Ÿå®æ—¶æ£€æµ‹â€œç›—åˆ·â€äº¤æ˜“ï¼Œ30 ç§’å†…é˜»æ–­
   3. èƒ½æºå…¬å¸å‘ç°è¾“ç”µçº¿è·¯ç¬æ—¶ç”µæµçªå¢ï¼Œå®šä½çŸ­è·¯éšæ‚£
   4. åŒ»å­¦å½±åƒ AI æŠŠâ€œç½•è§ç»“èŠ‚â€è‡ªåŠ¨åœˆå‡ºï¼Œä¾›æ”¾å°„ç§‘åŒ»ç”Ÿé‡ç‚¹å¤æ ¸

###  è‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-Supervised Learningï¼‰

- **å®šä¹‰**ï¼šè‡ªç›‘ç£å­¦ä¹ ä¸»è¦æ˜¯åˆ©ç”¨è¾…åŠ©ä»»åŠ¡(pretext)ä»å¤§è§„æ¨¡çš„æ— ç›‘ç£æ•°æ®ä¸­æŒ–æ˜è‡ªèº«çš„ç›‘ç£ä¿¡æ¯ï¼Œé€šè¿‡è¿™ç§æ„é€ çš„ç›‘ç£ä¿¡æ¯å¯¹ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå­¦ä¹ åˆ°å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ä»·å€¼çš„è¡¨å¾ã€‚

- **ç‰¹ç‚¹**ï¼š
  - **æ— éœ€äººå·¥æ ‡ç­¾**ï¼šåˆ©ç”¨æ•°æ®å†…åœ¨ç»“æ„è‡ªåŠ¨ç”Ÿæˆâ€œä¼ªæ ‡ç­¾â€ï¼Œæ•°æ®å‡†å¤‡æˆæœ¬æ¥è¿‘æ— ç›‘ç£ã€‚
  - **è¡¨å¾è´¨é‡é«˜**ï¼šé¢„è®­ç»ƒç›®æ ‡ä¸ä¸‹æ¸¸ä»»åŠ¡å…±äº«è¯­ä¹‰ä¿¡æ¯ï¼Œé€šå¸¸æ¯”ä¼ ç»Ÿæ— ç›‘ç£ç‰¹å¾æ›´å…·åˆ¤åˆ«åŠ›ã€‚
  - **è¿ç§»æ€§å¼º**ï¼šåŒä¸€å¥—è‡ªç›‘ç£é¢„è®­ç»ƒæƒé‡å¯å¾®è°ƒäºåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰å¤šç§ä»»åŠ¡ï¼Œæ˜¾è‘—æå‡å°æ ·æœ¬åœºæ™¯æ€§èƒ½ã€‚
  - **è¯„ä¼°é—´æ¥**ï¼šéœ€ä¾èµ–ä¸‹æ¸¸ä»»åŠ¡ç²¾åº¦æˆ–çº¿æ€§æ¢æµ‹ç²¾åº¦ä½œä¸ºä»£ç†æŒ‡æ ‡ï¼Œç¼ºä¹ç»Ÿä¸€ ground-truth è¡¡é‡è¡¨å¾ä¼˜åŠ£ã€‚

- **ä¸»è¦ä»»åŠ¡**ï¼š
  
  1. **åŸºäºä¸Šä¸‹æ–‡çš„è‡ªç›‘ç£å­¦ä¹ **ï¼šåŸºäºæ•°æ®æœ¬èº«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæˆ‘ä»¬å…¶å®å¯ä»¥æ„é€ å¾ˆå¤šä»»åŠ¡ï¼Œæ¯”å¦‚åœ¨ NLP é¢†åŸŸä¸­æœ€é‡è¦çš„ç®—æ³• Word2vec ã€‚Word2vec ä¸»è¦æ˜¯åˆ©ç”¨è¯­å¥çš„é¡ºåºï¼Œä¾‹å¦‚ CBOW é€šè¿‡å‰åçš„è¯æ¥é¢„æµ‹ä¸­é—´çš„è¯ï¼Œè€Œ Skip-Gram é€šè¿‡ä¸­é—´çš„è¯æ¥é¢„æµ‹å‰åçš„è¯ã€‚
  
     ![image-20251103184533888](.\data-annotation-images\image-20251103184533888.png)
  
     è€Œåœ¨å›¾åƒä¸­ï¼Œå¯ä»¥é€šè¿‡ä¸€ç§åä¸º Jigsawï¼ˆæ‹¼å›¾ï¼‰çš„æ–¹å¼æ¥æ„é€ è¾…åŠ©ä»»åŠ¡ã€‚æˆ‘ä»¬å¯ä»¥å°†ä¸€å¼ å›¾åˆ†æˆ 9 ä¸ªéƒ¨åˆ†ï¼Œç„¶åé€šè¿‡é¢„æµ‹è¿™å‡ ä¸ªéƒ¨åˆ†çš„ç›¸å¯¹ä½ç½®æ¥äº§ç”ŸæŸå¤±ã€‚æ¯”å¦‚æˆ‘ä»¬è¾“å…¥è¿™å¼ å›¾ä¸­çš„å°çŒ«çš„çœ¼ç›å’Œå³è€³æœµï¼ŒæœŸå¾…è®©æ¨¡å‹å­¦ä¹ åˆ°çŒ«çš„å³è€³æœµæ˜¯åœ¨è„¸éƒ¨çš„å³ä¸Šæ–¹çš„ï¼Œå¦‚æœæ¨¡å‹èƒ½å¾ˆå¥½çš„å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥è®¤ä¸ºæ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨å¾æ˜¯å…·æœ‰è¯­ä¹‰ä¿¡æ¯çš„ã€‚
  
     ![image-20251103191324420](.\data-annotation-images\image-20251103191324420.png)
  
     ![image-20251103191352885](.\data-annotation-images\image-20251103191352885.png)
  
  2. **åŸºäºæ—¶åºçš„è‡ªç›‘ç£å­¦ä¹ **ï¼šæœ€èƒ½ä½“ç°æ—¶åºçš„æ•°æ®ç±»å‹å°±æ˜¯è§†é¢‘äº†ï¼Œç¬¬ä¸€ç§æ€æƒ³æ˜¯åŸºäºå¸§çš„ç›¸ä¼¼æ€§ï¼Œå¯¹äºè§†é¢‘ä¸­çš„æ¯ä¸€å¸§ï¼Œç®€å•æ¥è¯´æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè§†é¢‘ä¸­çš„ç›¸é‚»å¸§ç‰¹å¾æ˜¯ç›¸ä¼¼çš„ï¼Œè€Œç›¸éš”è¾ƒè¿œçš„è§†é¢‘å¸§æ˜¯ä¸ç›¸ä¼¼çš„ï¼Œé€šè¿‡æ„å»ºè¿™ç§ç›¸ä¼¼ï¼ˆpositionï¼‰å’Œä¸ç›¸ä¼¼ï¼ˆnegativeï¼‰çš„æ ·æœ¬æ¥è¿›è¡Œè‡ªç›‘ç£çº¦æŸã€‚
  
     ![image-20251103190409881](.\data-annotation-images\image-20251103190409881.png)
  
     åœ¨å¤§é‡çš„æ— æ ‡ç­¾è§†é¢‘ä¸­è¿›è¡Œæ— ç›‘ç£è¿½è¸ªï¼Œè·å–å¤§é‡çš„ç‰©ä½“è¿½è¸ªæ¡†ã€‚é‚£ä¹ˆå¯¹äºä¸€ä¸ªç‰©ä½“è¿½è¸ªæ¡†åœ¨ä¸åŒå¸§çš„ç‰¹å¾åº”è¯¥æ˜¯ç›¸ä¼¼çš„ï¼ˆpositiveï¼‰ï¼Œè€Œå¯¹äºä¸åŒç‰©ä½“çš„è¿½è¸ªæ¡†ä¸­çš„ç‰¹å¾åº”è¯¥æ˜¯ä¸ç›¸ä¼¼çš„ï¼ˆnegativeï¼‰ã€‚
  
     ![image-20251103190621956](.\data-annotation-images\image-20251103190621956.png)
  
     è§†é¢‘çš„å…ˆåé¡ºåºä¹Ÿæ˜¯ä¸€ç§è‡ªç›‘ç£ä¿¡æ¯ã€‚æ¯”å¦‚ECCV 2016, Misra, I. [19] ç­‰äººæå‡ºåŸºäºé¡ºåºçº¦æŸçš„æ–¹æ³•ï¼Œå¯ä»¥ä»è§†é¢‘ä¸­é‡‡æ ·å‡ºæ­£ç¡®çš„è§†é¢‘åºåˆ—å’Œä¸æ­£ç¡®çš„è§†é¢‘åºåˆ—ï¼Œæ„é€ æˆæ­£è´Ÿæ ·æœ¬å¯¹ç„¶åè¿›è¡Œè®­ç»ƒã€‚ç®€è€Œè¨€ä¹‹ï¼Œå°±æ˜¯è®¾è®¡ä¸€ä¸ªæ¨¡å‹ï¼Œæ¥åˆ¤æ–­å½“å‰çš„è§†é¢‘åºåˆ—æ˜¯å¦æ˜¯æ­£ç¡®çš„é¡ºåºã€‚
  
     ![image-20251103190800128](.\data-annotation-images\image-20251103190800128.png)
  
  3. **åŸºäºå¯¹æ¯”çš„è‡ªç›‘ç£å­¦ä¹ **ï¼šé€šè¿‡å­¦ä¹ å¯¹ä¸¤ä¸ªäº‹ç‰©çš„ç›¸ä¼¼æˆ–ä¸ç›¸ä¼¼è¿›è¡Œç¼–ç æ¥æ„å»ºè¡¨å¾ã€‚å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œé€šè¿‡åœ¨è¾“å…¥æ ·æœ¬ä¹‹é—´è¿›è¡Œæ¯”è¾ƒæ¥å­¦ä¹ è¡¨ç¤ºã€‚å¯¹æ¯”å­¦ä¹ ä¸æ˜¯ä¸€æ¬¡ä»å•ä¸ªæ•°æ®æ ·æœ¬ä¸­å­¦ä¹ ä¿¡å·ï¼Œè€Œæ˜¯é€šè¿‡åœ¨ä¸åŒæ ·æœ¬ä¹‹é—´è¿›è¡Œæ¯”è¾ƒæ¥å­¦ä¹ ã€‚å¯¹æ¯”å­¦ä¹ é€šè¿‡åŒæ—¶æœ€å¤§åŒ–åŒä¸€å›¾åƒçš„ä¸åŒå˜æ¢è§†å›¾(ä¾‹å¦‚å‰ªè£ï¼Œç¿»è½¬ï¼Œé¢œè‰²å˜æ¢ç­‰)ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œä»¥åŠæœ€å°åŒ–ä¸åŒå›¾åƒçš„å˜æ¢è§†å›¾ä¹‹é—´çš„ä¸€è‡´æ€§æ¥å­¦ä¹ çš„ã€‚ ç®€å•æ¥è¯´ï¼Œå°±æ˜¯å¯¹æ¯”å­¦ä¹ è¦åšåˆ°ç›¸åŒçš„å›¾åƒç»è¿‡å„ç±»å˜æ¢ä¹‹åï¼Œä¾ç„¶èƒ½è¯†åˆ«å‡ºæ˜¯åŒä¸€å¼ å›¾åƒï¼Œæ‰€ä»¥è¦æœ€å¤§åŒ–å„ç±»å˜æ¢åå›¾åƒçš„ç›¸ä¼¼åº¦ï¼ˆå› ä¸ºéƒ½æ˜¯åŒä¸€ä¸ªå›¾åƒå¾—åˆ°çš„ï¼‰ã€‚
  
     ![image-20251103195711029](.\data-annotation-images\image-20251103195711029.png)
  
- **åº”ç”¨**ï¼š
  
  - **è®¡ç®—æœºè§†è§‰**ï¼šé€šè¿‡é¢„æµ‹å›¾åƒå—çš„ç›¸å¯¹ä½ç½®æˆ–æ—‹è½¬è§’åº¦ï¼Œè®­ç»ƒæ¨¡å‹å­¦ä¹ å›¾åƒçš„è¯­ä¹‰ç‰¹å¾ã€‚
  - **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šé‡‡ç”¨æ©ç è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ï¼Œé¢„æµ‹å¥å­ä¸­è¢«æ©ç›–çš„è¯è¯­ï¼Œç†è§£è¯æ±‡å’Œè¯­æ³•ç»“æ„ã€‚



## ğŸ¯ æ ‡æ³¨ç±»å‹ä¸åº”ç”¨

### å›¾åƒæ ‡æ³¨
- **åˆ†ç±»æ ‡æ³¨**ï¼šä¸ºå›¾åƒåˆ†é…ç±»åˆ«æ ‡ç­¾

  ![image-20251103200754374](.\data-annotation-images\image-20251103200754374.png)

- **ç›®æ ‡æ£€æµ‹**ï¼šæ ‡æ³¨ç‰©ä½“è¾¹ç•Œæ¡†å’Œç±»åˆ«

  ![image-20251103201439849](.\data-annotation-images\image-20251103201439849.png)

- **è¯­ä¹‰åˆ†å‰²**ï¼šåƒç´ çº§åˆ«çš„ç±»åˆ«æ ‡æ³¨

  ![image-20251103211142548](.\data-annotation-images\image-20251103211142548.png)

- **å®ä¾‹åˆ†å‰²**ï¼šåŒºåˆ†åŒç±»åˆ«çš„ä¸åŒå®ä¾‹

- **å…³é”®ç‚¹æ£€æµ‹**ï¼šæ ‡æ³¨äººä½“å§¿æ€ã€é¢éƒ¨ç‰¹å¾ç‚¹

### æ–‡æœ¬æ ‡æ³¨
- **æƒ…æ„Ÿåˆ†æ**ï¼šæ ‡æ³¨æ–‡æœ¬æƒ…æ„Ÿå€¾å‘

- **å‘½åå®ä½“è¯†åˆ«**ï¼šæ ‡æ³¨äººåã€åœ°åã€æœºæ„å

- **å…³ç³»æŠ½å–**ï¼šæ ‡æ³¨å®ä½“é—´çš„å…³ç³»

- **æ–‡æœ¬åˆ†ç±»**ï¼šä¸ºæ–‡æ¡£åˆ†é…ä¸»é¢˜æ ‡ç­¾

  ![image-20251103200837166](.\data-annotation-images\image-20251103200837166.png)

- **æœºå™¨ç¿»è¯‘**ï¼šæä¾›ç¿»è¯‘å¯¹ç…§

### éŸ³é¢‘æ ‡æ³¨
- **è¯­éŸ³è¯†åˆ«**ï¼šæ ‡æ³¨è¯­éŸ³å†…å®¹
- **éŸ³ä¹åˆ†ç±»**ï¼šæ ‡æ³¨éŸ³ä¹é£æ ¼ã€æƒ…ç»ª
- **ç¯å¢ƒå£°éŸ³**ï¼šæ ‡æ³¨å£°éŸ³äº‹ä»¶å’Œåœºæ™¯
- **è¯´è¯äººè¯†åˆ«**ï¼šæ ‡æ³¨è¯´è¯äººèº«ä»½

### è§†é¢‘æ ‡æ³¨
- **åŠ¨ä½œè¯†åˆ«**ï¼šæ ‡æ³¨è§†é¢‘ä¸­çš„è¡Œä¸ºåŠ¨ä½œ

- **åœºæ™¯åˆ†å‰²**ï¼šæ ‡æ³¨è§†é¢‘åœºæ™¯è¾¹ç•Œ

- **ç›®æ ‡è·Ÿè¸ª**ï¼šæ ‡æ³¨ç›®æ ‡åœ¨è§†é¢‘ä¸­çš„è½¨è¿¹

  ![7903c62a74f4f5ca0fa9a9bf27951c14](.\data-annotation-images\7903c62a74f4f5ca0fa9a9bf27951c14.gif)

- **äº‹ä»¶æ£€æµ‹**ï¼šæ ‡æ³¨ç‰¹å®šäº‹ä»¶çš„æ—¶é—´ç‚¹

### åœºæ™¯æµæ ‡æ³¨
æè¿°å›¾åƒæˆ–è§†é¢‘ä¸­æ¯ä¸ªåƒç´ åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„è¿åŠ¨çŸ¢é‡ï¼ŒåŒ…å«æ·±åº¦ä¿¡æ¯å’Œè¿åŠ¨è½¨è¿¹ã€‚
![image-20251103204942216](.\data-annotation-images\image-20251103204942216.png)
### å ç”¨åœ°å›¾æ ‡æ³¨
å°†ç¯å¢ƒåˆ’åˆ†ä¸ºç½‘æ ¼å•å…ƒï¼Œæ ‡æ³¨æ¯ä¸ªå•å…ƒæ˜¯å¦è¢«ç‰©ä½“å æ®ã€‚
![image-20251103205250837](.\data-annotation-images\image-20251103205250837.png)
### åœºæ™¯è¯´æ˜æ ‡æ³¨
ç”¨è‡ªç„¶è¯­è¨€æè¿°å›¾åƒæˆ–è§†é¢‘çš„å†…å®¹ï¼Œæä¾›é«˜å±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚

â€œåœ¨é˜³å…‰æ˜åªšçš„å…¬å›­é‡Œï¼Œå­©å­ä»¬æ­£åœ¨æ”¾é£ç­ï¼ŒèƒŒæ™¯æœ‰é•¿æ¤…å’Œæ ‘æœ¨ã€‚â€![scenedescription](.\data-annotation-images\scenedescription.png)

## ğŸ”¥å¸¸ç”¨çš„æ•°æ®æ ‡æ³¨æ–¹æ³•
ä¸ºäº†æé«˜æ ‡æ³¨æ•ˆç‡å’Œè´¨é‡ï¼Œç ”ç©¶è€…å¼€å‘äº†å¤šç§æ•°æ®æ ‡æ³¨æ–¹æ³•ï¼Œç»“åˆäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå‡å°‘äººå·¥å‚ä¸ã€‚

#### 1. è·¨åŸŸå­¦ä¹ ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆCross-Domain Pseudo-Labelingï¼‰

- **å®šä¹‰**ï¼šåˆ©ç”¨æ¥è‡ªä¸åŒé¢†åŸŸä½†ç›¸å…³çš„å·²æ ‡æ³¨æ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹åœ¨å½“å‰é¢†åŸŸçš„æœªæ ‡æ³¨æ•°æ®ä¸Šç”Ÿæˆä¼ªæ ‡ç­¾ã€‚
- **æ­¥éª¤**ï¼š
  1. **æºåŸŸè®­ç»ƒ**ï¼šåœ¨æºåŸŸï¼ˆå·²æ ‡æ³¨çš„ç›¸å…³é¢†åŸŸï¼‰ä¸Šè®­ç»ƒä¸€ä¸ªæ€§èƒ½è‰¯å¥½çš„æ¨¡å‹ã€‚
  2. **ç›®æ ‡åŸŸé¢„æµ‹**ï¼šä½¿ç”¨è¯¥æ¨¡å‹å¯¹ç›®æ ‡åŸŸï¼ˆæœªæ ‡æ³¨çš„æ•°æ®ï¼‰è¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆä¼ªæ ‡ç­¾ã€‚
  3. **ä¼ªæ ‡ç­¾ä¼˜åŒ–**ï¼šé€šè¿‡è¿­ä»£è‡ªè®­ç»ƒæˆ–ååŒè®­ç»ƒï¼Œé€æ­¥ä¿®æ­£ä¼ªæ ‡ç­¾çš„é”™è¯¯ï¼Œæé«˜å‡†ç¡®æ€§ã€‚
- **åº”ç”¨åœºæ™¯**ï¼š
  - **é¢†åŸŸé€‚åº”**ï¼šå½“ç›®æ ‡é¢†åŸŸç¼ºä¹æ ‡æ³¨æ•°æ®æ—¶ï¼Œåˆ©ç”¨æºé¢†åŸŸçš„çŸ¥è¯†è¿›è¡Œè¿ç§»ã€‚
  - **æ•°æ®å¢å¼º**ï¼šæ‰©å……ç›®æ ‡é¢†åŸŸçš„æ ‡æ³¨æ•°æ®ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
- **ç¤ºä¾‹**ï¼š
  - åˆ©ç”¨å·²æ ‡æ³¨çš„ç™½å¤©é©¾é©¶å›¾åƒè®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆå¤œé—´é©¾é©¶å›¾åƒçš„ä¼ªæ ‡ç­¾ã€‚

#### 2. ç¦»çº¿ç›®æ ‡è¿½è¸ªï¼ˆOffline Object Trackingï¼‰

- **å®šä¹‰**ï¼šåœ¨ç¦»çº¿æƒ…å†µä¸‹ï¼Œå¯¹è§†é¢‘åºåˆ—ä¸­çš„ç›®æ ‡è¿›è¡Œè¿½è¸ªå’Œæ ‡æ³¨ï¼Œè·å–å…¶è¿åŠ¨è½¨è¿¹ã€‚
- **æµç¨‹**ï¼š
  1. **æ£€æµ‹é˜¶æ®µ**ï¼šåœ¨æ¯å¸§ä¸­æ£€æµ‹ç›®æ ‡ç‰©ä½“çš„ä½ç½®ã€‚
  2. **å…³è”é˜¶æ®µ**ï¼šæ ¹æ®ç›®æ ‡çš„ç‰¹å¾ï¼ˆå¦‚å¤–è§‚ã€è¿åŠ¨ï¼‰å°†ä¸åŒå¸§ä¸­çš„æ£€æµ‹ç»“æœå…³è”èµ·æ¥ã€‚
  3. **è½¨è¿¹ç”Ÿæˆ**ï¼šä¸ºæ¯ä¸ªç›®æ ‡ç”Ÿæˆå®Œæ•´çš„è¿åŠ¨è½¨è¿¹ï¼Œåˆ†é…å”¯ä¸€ IDã€‚
- **ä¼˜åŠ¿**ï¼š
  - **é«˜ç²¾åº¦**ï¼šåˆ©ç”¨å…¨å±€ä¿¡æ¯ï¼Œå¯¹ç›®æ ‡è¿›è¡Œæ›´å‡†ç¡®çš„å…³è”ã€‚
  - **çº é”™èƒ½åŠ›**ï¼šå¯ä»¥é€šè¿‡åå¤„ç†æ­¥éª¤ï¼Œä¿®æ­£è¿½è¸ªè¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚
- **åº”ç”¨åœºæ™¯**ï¼š
  - **è§†é¢‘åˆ†æ**ï¼šåˆ†æç›‘æ§å½•åƒä¸­äººå‘˜æˆ–è½¦è¾†çš„è¡Œä¸ºã€‚
  - **æ•°æ®æ ‡æ³¨**ï¼šä¸ºè§†é¢‘æ•°æ®ç”Ÿæˆç›®æ ‡è¿½è¸ªçš„æ ‡æ³¨ï¼Œç”¨äºè®­ç»ƒè¿½è¸ªæ¨¡å‹ã€‚
- **ç¤ºä¾‹**ï¼š
  - æ ‡æ³¨ä¸€æ®µäº¤é€šè§†é¢‘ä¸­æ‰€æœ‰è½¦è¾†çš„è½¨è¿¹ï¼Œç”¨äºè®­ç»ƒè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€‚

#### 3. ç¦»çº¿åœºæ™¯æµä¼°è®¡ï¼ˆOffline Scene Flow Estimationï¼‰

- **å®šä¹‰**ï¼šä»é¢„å…ˆé‡‡é›†çš„å›¾åƒæˆ–è§†é¢‘åºåˆ—ä¸­ï¼Œè®¡ç®—æ¯ä¸ªåƒç´ çš„ä¸‰ç»´è¿åŠ¨çŸ¢é‡ã€‚
- **æ–¹æ³•**ï¼š
  - **ç«‹ä½“è§†è§‰**ï¼šåˆ©ç”¨åŒç›®ç›¸æœºè·å–çš„æ·±åº¦ä¿¡æ¯ï¼Œç»“åˆå…‰æµæ³•è®¡ç®—åœºæ™¯æµã€‚
  - **æ·±åº¦å­¦ä¹ **ï¼šè®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»å›¾åƒå¯¹ä¸­ç›´æ¥é¢„æµ‹åœºæ™¯æµã€‚
- **åº”ç”¨**ï¼š
  - **ä¸‰ç»´é‡å»º**ï¼šæ„å»ºåŠ¨æ€åœºæ™¯çš„ä¸‰ç»´æ¨¡å‹ã€‚
  - **è¿åŠ¨åˆ†æ**ï¼šåˆ†æåœºæ™¯ä¸­ç‰©ä½“çš„è¿åŠ¨æ¨¡å¼ã€‚
- **ç¤ºä¾‹**ï¼š
  - ä¼°è®¡è¡Œäººè¿‡é©¬è·¯æ—¶çš„è¿åŠ¨çŸ¢é‡ï¼Œé¢„æµ‹å…¶æœªæ¥ä½ç½®ã€‚

#### 4. åœºæ™¯åˆ†ç±»ï¼ˆScene Classificationï¼‰

- **å®šä¹‰**ï¼šæ ¹æ®å›¾åƒæˆ–è§†é¢‘çš„å†…å®¹ï¼Œå°†å…¶å½’ç±»åˆ°é¢„å®šä¹‰çš„åœºæ™¯ç±»åˆ«ä¸­ã€‚
- **æ–¹æ³•**ï¼š
  - **åŸºäºç‰¹å¾**ï¼šæå–å›¾åƒçš„çº¹ç†ã€é¢œè‰²ã€å½¢çŠ¶ç­‰ç‰¹å¾ï¼Œä½¿ç”¨åˆ†ç±»å™¨ï¼ˆå¦‚ SVMã€éšæœºæ£®æ—ï¼‰è¿›è¡Œåˆ†ç±»ã€‚
  - **åŸºäºæ·±åº¦å­¦ä¹ **ï¼šä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç­‰æ¨¡å‹ï¼Œè‡ªåŠ¨å­¦ä¹ å›¾åƒçš„ç‰¹å¾ï¼Œè¿›è¡Œåˆ†ç±»ã€‚
- **åº”ç”¨åœºæ™¯**ï¼š
  - **å›¾åƒç®¡ç†**ï¼šå¯¹å¤§é‡å›¾åƒè¿›è¡Œè‡ªåŠ¨åˆ†ç±»ï¼Œæ–¹ä¾¿æ£€ç´¢å’Œç®¡ç†ã€‚
  - **æ™ºèƒ½æ¨è**ï¼šæ ¹æ®ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡ï¼Œæ¨èç›¸å…³çš„æ ‡ç­¾æˆ–å†…å®¹ã€‚

#### 5. ä¸»åŠ¨å­¦ä¹ ï¼ˆActive Learningï¼‰

- **å®šä¹‰**ï¼šé€šè¿‡æ™ºèƒ½é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œä»¥æœ€å°çš„æ ‡æ³¨é‡è·å¾—æœ€å¤§çš„æ¨¡å‹æ€§èƒ½æå‡ã€‚
- **æµç¨‹**ï¼š
  1. **åˆå§‹è®­ç»ƒ**ï¼šä½¿ç”¨å°‘é‡å·²æ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚
  2. **æ ·æœ¬é€‰æ‹©**ï¼šæ¨¡å‹ä»å¤§é‡æœªæ ‡æ³¨æ•°æ®ä¸­ï¼Œé€‰æ‹©ä¸ç¡®å®šæ€§é«˜æˆ–ä¿¡æ¯é‡å¤§çš„æ ·æœ¬ã€‚
  3. **äººå·¥æ ‡æ³¨**ï¼šä¸“å®¶å¯¹é€‰ä¸­çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ã€‚
  4. **æ¨¡å‹æ›´æ–°**ï¼šå°†æ–°æ ‡æ³¨çš„æ•°æ®åŠ å…¥è®­ç»ƒé›†ï¼Œé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
- **ä¼˜åŠ¿**ï¼š
  - **é«˜æ•ˆæ€§**ï¼šå‡å°‘å†—ä½™æ ‡æ³¨ï¼Œé™ä½æ ‡æ³¨æˆæœ¬ã€‚
  - **é€‚åº”æ€§**ï¼šæ¨¡å‹æ€§èƒ½éšç€æ ‡æ³¨æ•°æ®çš„å¢åŠ è€Œå¿«é€Ÿæå‡ã€‚
- **åº”ç”¨åœºæ™¯**ï¼š
  - **åŒ»ç–—å›¾åƒ**ï¼šç­›é€‰å‡ºéœ€è¦ä¸“å®¶è¯Šæ–­çš„ç–‘éš¾ç—…ä¾‹ã€‚
  - **æ–‡æœ¬åˆ†ç±»**ï¼šæŒ‘é€‰å‡ºå¯¹æ¨¡å‹æœ€æœ‰å¸®åŠ©çš„æ–‡æœ¬è¿›è¡Œæ ‡æ³¨ã€‚

#### 6. ä¼—åŒ…æ ‡æ³¨ï¼ˆCrowdsourcingï¼‰

- **å®šä¹‰**ï¼šå°†æ ‡æ³¨ä»»åŠ¡åˆ†å‘ç»™å¤§é‡éä¸“ä¸šçš„æ ‡æ³¨è€…ï¼ˆä¼—åŒ…å·¥äººï¼‰ï¼Œé€šè¿‡æ±‡èšä¼—äººçš„åŠ›é‡å®Œæˆæ ‡æ³¨ã€‚
- **å¹³å°**ï¼š
  - **Amazon Mechanical Turk**ï¼šä¸€ä¸ªæµè¡Œçš„ä¼—åŒ…å¹³å°ï¼Œå¯ä»¥å‘å¸ƒå¾®ä»»åŠ¡ã€‚
  - **å›½å†…å¹³å°**ï¼šå¦‚â€œæ•°æ®å ‚â€ã€â€œç™¾åº¦ä¼—æµ‹â€ã€‚
- **è´¨é‡æ§åˆ¶**ï¼š
  - **å¤šé‡æ ‡æ³¨**ï¼šåŒä¸€æ•°æ®ç”±å¤šä¸ªæ ‡æ³¨è€…æ ‡æ³¨ï¼Œé€šè¿‡æŠ•ç¥¨æˆ–ä¸€è‡´æ€§æ£€æŸ¥ç¡®ä¿è´¨é‡ã€‚
  - **æµ‹è¯•é¢˜**ï¼šæ’å…¥å·²çŸ¥ç­”æ¡ˆçš„æµ‹è¯•é¢˜ï¼Œè¯„ä¼°æ ‡æ³¨è€…çš„å¯é æ€§ã€‚
- **é€‚ç”¨æ€§**ï¼š
  - **ç®€å•ä»»åŠ¡**ï¼šå¦‚å›¾åƒåˆ†ç±»ã€è¾¹ç•Œæ¡†æ ‡æ³¨ã€‚
  - **æˆæœ¬æ•æ„Ÿ**ï¼šåœ¨é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè·å–å¤§é‡æ ‡æ³¨æ•°æ®ã€‚
- **ç¤ºä¾‹**ï¼š
  - æ”¶é›†å…¬ä¼—å¯¹ç¤¾äº¤åª’ä½“å¸–å­çš„æƒ…æ„Ÿæ ‡æ³¨ï¼Œç”¨äºæƒ…æ„Ÿåˆ†ææ¨¡å‹ã€‚

#### 7. åˆæˆæ•°æ®ç”Ÿæˆï¼ˆSynthetic Data Generationï¼‰

- **å®šä¹‰**ï¼šåˆ©ç”¨è®¡ç®—æœºæ¨¡æ‹Ÿæˆ–ç®—æ³•ç”Ÿæˆå¸¦æœ‰æ ‡æ³¨çš„åˆæˆæ•°æ®ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ã€‚
- **æ–¹æ³•**ï¼š
  - **å›¾å½¢æ¸²æŸ“**ï¼šä½¿ç”¨ 3D å»ºæ¨¡å’Œæ¸²æŸ“æŠ€æœ¯ï¼Œç”Ÿæˆé€¼çœŸçš„å›¾åƒå’Œè§†é¢‘ã€‚
  - **ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰**ï¼šè®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œäº§ç”Ÿä¸çœŸå®æ•°æ®ç›¸ä¼¼çš„åˆæˆæ•°æ®ã€‚
- **ä¼˜åŠ¿**ï¼š
  - **æ— é™æ•°æ®**ï¼šå¯ä»¥ç”Ÿæˆä»»æ„æ•°é‡çš„æ ‡æ³¨æ•°æ®ã€‚
  - **å¤šæ ·æ€§**ï¼šæ§åˆ¶æ•°æ®çš„åˆ†å¸ƒå’Œç‰¹å¾ï¼Œè¦†ç›–æ›´å¤šåœºæ™¯ã€‚
- **æŒ‘æˆ˜**ï¼š
  - **çœŸå®æ„Ÿ**ï¼šåˆæˆæ•°æ®éœ€è¦ä¸çœŸå®æ•°æ®è¶³å¤Ÿç›¸ä¼¼ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
  - **é¢†åŸŸå·®è·**ï¼šåˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ä¹‹é—´å¯èƒ½å­˜åœ¨åˆ†å¸ƒå·®å¼‚ã€‚
- **åº”ç”¨åœºæ™¯**ï¼š
  - **è‡ªåŠ¨é©¾é©¶**ï¼šç”Ÿæˆå„ç§äº¤é€šåœºæ™¯ï¼Œè®­ç»ƒæ„ŸçŸ¥æ¨¡å‹ã€‚
  - **æœºå™¨äººæŠ“å–**ï¼šæ¨¡æ‹Ÿç‰©ä½“å’Œæœºæ¢°è‡‚çš„äº¤äº’ï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚
- **ç¤ºä¾‹**ï¼š
  - ä½¿ç”¨æ¸¸æˆå¼•æ“ç”Ÿæˆä¸åŒå¤©æ°”å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„é“è·¯åœºæ™¯ï¼Œç”¨äºè®­ç»ƒè½¦è¾†æ£€æµ‹æ¨¡å‹ã€‚

## ğŸ› ï¸ æ ‡æ³¨å·¥å…·ä¸å¹³å°

### å¼€æºæ ‡æ³¨å·¥å…·

#### å›¾åƒæ ‡æ³¨å·¥å…·
```python
# LabelImg - ç›®æ ‡æ£€æµ‹æ ‡æ³¨
# å®‰è£…ï¼špip install labelImg
# ä½¿ç”¨ï¼šlabelImg

# åŸºäºPythonçš„è‡ªå®šä¹‰æ ‡æ³¨å·¥å…·
import tkinter as tk
from tkinter import filedialog, messagebox
from PIL import Image, ImageTk
import json
import os

class ImageAnnotationTool:
    def __init__(self, root):
        self.root = root
        self.root.title("å›¾åƒæ ‡æ³¨å·¥å…·")
        self.root.geometry("1200x800")
        
        self.image_path = ""
        self.annotations = []
        self.current_bbox = None
        self.start_x = 0
        self.start_y = 0
        
        self.setup_ui()
    
    def setup_ui(self):
        # èœå•æ 
        menubar = tk.Menu(self.root)
        self.root.config(menu=menubar)
        
        file_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="æ–‡ä»¶", menu=file_menu)
        file_menu.add_command(label="æ‰“å¼€å›¾åƒ", command=self.load_image)
        file_menu.add_command(label="ä¿å­˜æ ‡æ³¨", command=self.save_annotations)
        file_menu.add_command(label="åŠ è½½æ ‡æ³¨", command=self.load_annotations)
        
        # ä¸»æ¡†æ¶
        main_frame = tk.Frame(self.root)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # å·¦ä¾§ï¼šå›¾åƒæ˜¾ç¤º
        self.image_frame = tk.Frame(main_frame, bg="white")
        self.image_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        
        self.canvas = tk.Canvas(self.image_frame, bg="white")
        self.canvas.pack(fill=tk.BOTH, expand=True)
        
        # ç»‘å®šé¼ æ ‡äº‹ä»¶
        self.canvas.bind("<Button-1>", self.start_bbox)
        self.canvas.bind("<B1-Motion>", self.draw_bbox)
        self.canvas.bind("<ButtonRelease-1>", self.end_bbox)
        
        # å³ä¾§ï¼šæ§åˆ¶é¢æ¿
        control_frame = tk.Frame(main_frame, width=300)
        control_frame.pack(side=tk.RIGHT, fill=tk.Y, padx=(10, 0))
        control_frame.pack_propagate(False)
        
        # ç±»åˆ«é€‰æ‹©
        tk.Label(control_frame, text="é€‰æ‹©ç±»åˆ«:", font=("Arial", 12)).pack(pady=5)
        self.class_var = tk.StringVar(value="person")
        self.class_entry = tk.Entry(control_frame, textvariable=self.class_var)
        self.class_entry.pack(pady=5, fill=tk.X)
        
        # é¢„å®šä¹‰ç±»åˆ«
        classes = ["person", "car", "bike", "dog", "cat"]
        for cls in classes:
            btn = tk.Button(control_frame, text=cls, 
                          command=lambda c=cls: self.class_var.set(c))
            btn.pack(pady=2, fill=tk.X)
        
        # æ ‡æ³¨åˆ—è¡¨
        tk.Label(control_frame, text="æ ‡æ³¨åˆ—è¡¨:", font=("Arial", 12)).pack(pady=(20, 5))
        
        self.annotation_listbox = tk.Listbox(control_frame, height=10)
        self.annotation_listbox.pack(fill=tk.BOTH, expand=True, pady=5)
        
        # åˆ é™¤æŒ‰é’®
        tk.Button(control_frame, text="åˆ é™¤é€‰ä¸­æ ‡æ³¨", 
                 command=self.delete_annotation).pack(pady=5, fill=tk.X)
        
        # æ¸…ç©ºæŒ‰é’®
        tk.Button(control_frame, text="æ¸…ç©ºæ‰€æœ‰æ ‡æ³¨", 
                 command=self.clear_annotations).pack(pady=5, fill=tk.X)
    
    def load_image(self):
        file_path = filedialog.askopenfilename(
            filetypes=[("Image files", "*.jpg *.jpeg *.png *.bmp *.gif")]
        )
        if file_path:
            self.image_path = file_path
            self.display_image()
    
    def display_image(self):
        if self.image_path:
            image = Image.open(self.image_path)
            # è°ƒæ•´å›¾åƒå¤§å°ä»¥é€‚åº”ç”»å¸ƒ
            canvas_width = self.canvas.winfo_width()
            canvas_height = self.canvas.winfo_height()
            
            if canvas_width > 1 and canvas_height > 1:
                image.thumbnail((canvas_width, canvas_height), Image.Resampling.LANCZOS)
                self.photo = ImageTk.PhotoImage(image)
                self.canvas.delete("all")
                self.canvas.create_image(0, 0, anchor=tk.NW, image=self.photo)
                self.canvas.config(scrollregion=self.canvas.bbox("all"))
    
    def start_bbox(self, event):
        self.start_x = self.canvas.canvasx(event.x)
        self.start_y = self.canvas.canvasy(event.y)
        
        if self.current_bbox:
            self.canvas.delete(self.current_bbox)
        
        self.current_bbox = self.canvas.create_rectangle(
            self.start_x, self.start_y, self.start_x, self.start_y,
            outline="red", width=2
        )
    
    def draw_bbox(self, event):
        cur_x = self.canvas.canvasx(event.x)
        cur_y = self.canvas.canvasy(event.y)
        
        self.canvas.coords(self.current_bbox, self.start_x, self.start_y, cur_x, cur_y)
    
    def end_bbox(self, event):
        end_x = self.canvas.canvasx(event.x)
        end_y = self.canvas.canvasy(event.y)
        
        # ç¡®ä¿è¾¹ç•Œæ¡†æœ‰æ•ˆ
        if abs(end_x - self.start_x) > 5 and abs(end_y - self.start_y) > 5:
            annotation = {
                "class": self.class_var.get(),
                "bbox": [
                    min(self.start_x, end_x),
                    min(self.start_y, end_y),
                    max(self.start_x, end_x),
                    max(self.start_y, end_y)
                ]
            }
            self.annotations.append(annotation)
            self.update_annotation_list()
        else:
            self.canvas.delete(self.current_bbox)
        
        self.current_bbox = None
    
    def update_annotation_list(self):
        self.annotation_listbox.delete(0, tk.END)
        for i, ann in enumerate(self.annotations):
            self.annotation_listbox.insert(tk.END, 
                f"{i+1}. {ann['class']}: {ann['bbox']}")
    
    def delete_annotation(self):
        selection = self.annotation_listbox.curselection()
        if selection:
            index = selection[0]
            del self.annotations[index]
            self.update_annotation_list()
            self.redraw_annotations()
    
    def clear_annotations(self):
        self.annotations = []
        self.update_annotation_list()
        self.canvas.delete("bbox")
    
    def redraw_annotations(self):
        self.canvas.delete("bbox")
        for ann in self.annotations:
            bbox = ann["bbox"]
            self.canvas.create_rectangle(
                bbox[0], bbox[1], bbox[2], bbox[3],
                outline="red", width=2, tags="bbox"
            )
    
    def save_annotations(self):
        if not self.image_path:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆåŠ è½½å›¾åƒ")
            return
        
        annotation_file = self.image_path.rsplit('.', 1)[0] + '.json'
        annotation_data = {
            "image_path": self.image_path,
            "annotations": self.annotations
        }
        
        with open(annotation_file, 'w', encoding='utf-8') as f:
            json.dump(annotation_data, f, indent=2, ensure_ascii=False)
        
        messagebox.showinfo("æˆåŠŸ", f"æ ‡æ³¨å·²ä¿å­˜è‡³ {annotation_file}")
    
    def load_annotations(self):
        if not self.image_path:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆåŠ è½½å›¾åƒ")
            return
        
        annotation_file = self.image_path.rsplit('.', 1)[0] + '.json'
        if os.path.exists(annotation_file):
            with open(annotation_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.annotations = data.get("annotations", [])
                self.update_annotation_list()
                self.redraw_annotations()
        else:
            messagebox.showinfo("ä¿¡æ¯", "æœªæ‰¾åˆ°å¯¹åº”çš„æ ‡æ³¨æ–‡ä»¶")

# å¯åŠ¨æ ‡æ³¨å·¥å…·
if __name__ == "__main__":
    root = tk.Tk()
    app = ImageAnnotationTool(root)
    root.mainloop()
```

#### æ–‡æœ¬æ ‡æ³¨å·¥å…·
```python
import streamlit as st
import pandas as pd
import json
from datetime import datetime

class TextAnnotationTool:
    def __init__(self):
        self.init_session_state()
    
    def init_session_state(self):
        if 'texts' not in st.session_state:
            st.session_state.texts = []
        if 'annotations' not in st.session_state:
            st.session_state.annotations = {}
        if 'current_index' not in st.session_state:
            st.session_state.current_index = 0
    
    def load_texts(self, file):
        """åŠ è½½å¾…æ ‡æ³¨æ–‡æœ¬"""
        if file.type == "text/csv":
            df = pd.read_csv(file)
            if 'text' in df.columns:
                st.session_state.texts = df['text'].tolist()
            else:
                st.error("CSVæ–‡ä»¶å¿…é¡»åŒ…å«'text'åˆ—")
        elif file.type == "application/json":
            data = json.load(file)
            if isinstance(data, list):
                st.session_state.texts = data
            else:
                st.error("JSONæ–‡ä»¶å¿…é¡»æ˜¯æ–‡æœ¬åˆ—è¡¨")
        else:
            # çº¯æ–‡æœ¬æ–‡ä»¶
            content = str(file.read(), "utf-8")
            st.session_state.texts = content.split('\n')
        
        st.session_state.current_index = 0
        st.success(f"æˆåŠŸåŠ è½½ {len(st.session_state.texts)} æ¡æ–‡æœ¬")
    
    def annotate_sentiment(self):
        """æƒ…æ„Ÿåˆ†ææ ‡æ³¨"""
        if not st.session_state.texts:
            st.warning("è¯·å…ˆä¸Šä¼ æ–‡æœ¬æ–‡ä»¶")
            return
        
        st.subheader("æƒ…æ„Ÿåˆ†ææ ‡æ³¨")
        
        # æ˜¾ç¤ºå½“å‰æ–‡æœ¬
        current_text = st.session_state.texts[st.session_state.current_index]
        st.text_area("å½“å‰æ–‡æœ¬", current_text, height=100, disabled=True)
        
        # æ ‡æ³¨é€‰é¡¹
        sentiment = st.radio(
            "é€‰æ‹©æƒ…æ„Ÿæ ‡ç­¾",
            ["ç§¯æ", "æ¶ˆæ", "ä¸­æ€§"],
            key=f"sentiment_{st.session_state.current_index}"
        )
        
        # ç½®ä¿¡åº¦
        confidence = st.slider("æ ‡æ³¨ç½®ä¿¡åº¦", 0.0, 1.0, 0.8, 0.1)
        
        # ä¿å­˜æ ‡æ³¨
        if st.button("ä¿å­˜æ ‡æ³¨"):
            text_id = st.session_state.current_index
            st.session_state.annotations[text_id] = {
                "text": current_text,
                "sentiment": sentiment,
                "confidence": confidence,
                "timestamp": datetime.now().isoformat()
            }
            st.success("æ ‡æ³¨å·²ä¿å­˜")
            
            # è‡ªåŠ¨è·³è½¬åˆ°ä¸‹ä¸€æ¡
            if st.session_state.current_index < len(st.session_state.texts) - 1:
                st.session_state.current_index += 1
                st.experimental_rerun()
    
    def annotate_ner(self):
        """å‘½åå®ä½“è¯†åˆ«æ ‡æ³¨"""
        if not st.session_state.texts:
            st.warning("è¯·å…ˆä¸Šä¼ æ–‡æœ¬æ–‡ä»¶")
            return
        
        st.subheader("å‘½åå®ä½“è¯†åˆ«æ ‡æ³¨")
        
        current_text = st.session_state.texts[st.session_state.current_index]
        st.text_area("å½“å‰æ–‡æœ¬", current_text, height=100, disabled=True)
        
        # å®ä½“ç±»å‹
        entity_types = ["PERSON", "ORG", "GPE", "MONEY", "DATE", "TIME"]
        
        # å®ä½“æ ‡æ³¨
        entities = []
        
        st.write("æ ‡æ³¨å®ä½“ï¼š")
        for i in range(5):  # æœ€å¤šæ ‡æ³¨5ä¸ªå®ä½“
            col1, col2, col3, col4 = st.columns([2, 2, 1, 1])
            
            with col1:
                entity_text = st.text_input(f"å®ä½“æ–‡æœ¬ {i+1}", key=f"entity_text_{i}")
            
            with col2:
                entity_type = st.selectbox(f"å®ä½“ç±»å‹ {i+1}", 
                                         [""] + entity_types, key=f"entity_type_{i}")
            
            with col3:
                start_pos = st.number_input(f"å¼€å§‹ä½ç½® {i+1}", 
                                          min_value=0, key=f"start_{i}")
            
            with col4:
                end_pos = st.number_input(f"ç»“æŸä½ç½® {i+1}", 
                                        min_value=0, key=f"end_{i}")
            
            if entity_text and entity_type:
                entities.append({
                    "text": entity_text,
                    "type": entity_type,
                    "start": int(start_pos),
                    "end": int(end_pos)
                })
        
        # ä¿å­˜æ ‡æ³¨
        if st.button("ä¿å­˜NERæ ‡æ³¨"):
            text_id = st.session_state.current_index
            st.session_state.annotations[text_id] = {
                "text": current_text,
                "entities": entities,
                "timestamp": datetime.now().isoformat()
            }
            st.success("NERæ ‡æ³¨å·²ä¿å­˜")
    
    def navigation(self):
        """å¯¼èˆªæ§åˆ¶"""
        if st.session_state.texts:
            col1, col2, col3 = st.columns([1, 2, 1])
            
            with col1:
                if st.button("ä¸Šä¸€æ¡"):
                    if st.session_state.current_index > 0:
                        st.session_state.current_index -= 1
                        st.experimental_rerun()
            
            with col2:
                st.write(f"å½“å‰è¿›åº¦: {st.session_state.current_index + 1} / {len(st.session_state.texts)}")
            
            with col3:
                if st.button("ä¸‹ä¸€æ¡"):
                    if st.session_state.current_index < len(st.session_state.texts) - 1:
                        st.session_state.current_index += 1
                        st.experimental_rerun()
    
    def export_annotations(self):
        """å¯¼å‡ºæ ‡æ³¨ç»“æœ"""
        if st.session_state.annotations:
            annotations_json = json.dumps(st.session_state.annotations, 
                                        indent=2, ensure_ascii=False)
            
            st.download_button(
                label="ä¸‹è½½æ ‡æ³¨ç»“æœ",
                data=annotations_json,
                file_name=f"annotations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
    
    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        st.title("æ–‡æœ¬æ ‡æ³¨å·¥å…·")
        
        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶",
            type=['txt', 'csv', 'json'],
            help="æ”¯æŒtxtã€csvã€jsonæ ¼å¼"
        )
        
        if uploaded_file:
            self.load_texts(uploaded_file)
        
        # æ ‡æ³¨ç±»å‹é€‰æ‹©
        annotation_type = st.sidebar.selectbox(
            "é€‰æ‹©æ ‡æ³¨ç±»å‹",
            ["æƒ…æ„Ÿåˆ†æ", "å‘½åå®ä½“è¯†åˆ«"]
        )
        
        # å¯¼èˆª
        self.navigation()
        
        # æ ‡æ³¨ç•Œé¢
        if annotation_type == "æƒ…æ„Ÿåˆ†æ":
            self.annotate_sentiment()
        elif annotation_type == "å‘½åå®ä½“è¯†åˆ«":
            self.annotate_ner()
        
        # å¯¼å‡ºåŠŸèƒ½
        st.sidebar.subheader("å¯¼å‡ºæ ‡æ³¨")
        self.export_annotations()
        
        # æ˜¾ç¤ºæ ‡æ³¨ç»Ÿè®¡
        if st.session_state.annotations:
            st.sidebar.subheader("æ ‡æ³¨ç»Ÿè®¡")
            st.sidebar.write(f"å·²æ ‡æ³¨: {len(st.session_state.annotations)} æ¡")
            st.sidebar.write(f"å‰©ä½™: {len(st.session_state.texts) - len(st.session_state.annotations)} æ¡")

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    app = TextAnnotationTool()
    app.run()
```

### å•†ä¸šæ ‡æ³¨å¹³å°

#### ä¼—åŒ…æ ‡æ³¨å¹³å°
- **Amazon Mechanical Turk**ï¼šäºšé©¬é€Šä¼—åŒ…å¹³å°
- **Figure Eight (now Appen)**ï¼šä¸“ä¸šæ•°æ®æ ‡æ³¨æœåŠ¡
- **Labelbox**ï¼šç«¯åˆ°ç«¯æ ‡æ³¨å¹³å°
- **Scale AI**ï¼šAIè®­ç»ƒæ•°æ®å¹³å°

#### ä¼ä¸šçº§æ ‡æ³¨è§£å†³æ–¹æ¡ˆ
- **Supervisely**ï¼šè®¡ç®—æœºè§†è§‰æ ‡æ³¨å¹³å°
- **Hasty.ai**ï¼šAIè¾…åŠ©æ ‡æ³¨å·¥å…·
- **V7 Labs**ï¼šåŒ»å­¦å›¾åƒæ ‡æ³¨ä¸“ç”¨
- **Dataloop**ï¼šæ•°æ®ç®¡ç†å’Œæ ‡æ³¨å¹³å°

## ğŸ“Š æ ‡æ³¨è´¨é‡æ§åˆ¶

### è´¨é‡è¯„ä¼°æŒ‡æ ‡
```python
import numpy as np
from sklearn.metrics import cohen_kappa_score, accuracy_score
import pandas as pd

class AnnotationQualityControl:
    def __init__(self):
        pass
    
    def inter_annotator_agreement(self, annotations_df):
        """è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§"""
        annotators = annotations_df['annotator'].unique()
        agreements = {}
        
        for i, ann1 in enumerate(annotators):
            for ann2 in annotators[i+1:]:
                # è·å–ä¸¤ä¸ªæ ‡æ³¨è€…çš„å…±åŒæ ‡æ³¨
                ann1_data = annotations_df[annotations_df['annotator'] == ann1]
                ann2_data = annotations_df[annotations_df['annotator'] == ann2]
                
                # åˆå¹¶å…±åŒé¡¹ç›®
                common = pd.merge(ann1_data, ann2_data, on='item_id', 
                                suffixes=('_1', '_2'))
                
                if len(common) > 0:
                    # è®¡ç®—Cohen's Kappa
                    kappa = cohen_kappa_score(common['label_1'], common['label_2'])
                    accuracy = accuracy_score(common['label_1'], common['label_2'])
                    
                    agreements[f"{ann1}_vs_{ann2}"] = {
                        'kappa': kappa,
                        'accuracy': accuracy,
                        'common_items': len(common)
                    }
        
        return agreements
    
    def calculate_annotation_time_stats(self, annotations_df):
        """è®¡ç®—æ ‡æ³¨æ—¶é—´ç»Ÿè®¡"""
        if 'start_time' in annotations_df.columns and 'end_time' in annotations_df.columns:
            annotations_df['duration'] = (
                pd.to_datetime(annotations_df['end_time']) - 
                pd.to_datetime(annotations_df['start_time'])
            ).dt.total_seconds()
            
            stats = {
                'mean_duration': annotations_df['duration'].mean(),
                'median_duration': annotations_df['duration'].median(),
                'std_duration': annotations_df['duration'].std(),
                'min_duration': annotations_df['duration'].min(),
                'max_duration': annotations_df['duration'].max()
            }
            
            # æ£€æµ‹å¼‚å¸¸å¿«é€Ÿæˆ–ç¼“æ…¢çš„æ ‡æ³¨
            Q1 = annotations_df['duration'].quantile(0.25)
            Q3 = annotations_df['duration'].quantile(0.75)
            IQR = Q3 - Q1
            
            outliers = annotations_df[
                (annotations_df['duration'] < Q1 - 1.5 * IQR) |
                (annotations_df['duration'] > Q3 + 1.5 * IQR)
            ]
            
            stats['outlier_count'] = len(outliers)
            stats['outlier_percentage'] = len(outliers) / len(annotations_df) * 100
            
            return stats
        else:
            return {"error": "ç¼ºå°‘æ—¶é—´æˆ³ä¿¡æ¯"}
    
    def detect_annotation_patterns(self, annotations_df):
        """æ£€æµ‹æ ‡æ³¨æ¨¡å¼å¼‚å¸¸"""
        patterns = {}
        
        # æ£€æµ‹æ ‡æ³¨åˆ†å¸ƒ
        label_distribution = annotations_df['label'].value_counts(normalize=True)
        patterns['label_distribution'] = label_distribution.to_dict()
        
        # æ£€æµ‹æ ‡æ³¨è€…åå¥½
        annotator_patterns = annotations_df.groupby('annotator')['label'].value_counts(normalize=True)
        patterns['annotator_preferences'] = annotator_patterns.to_dict()
        
        # æ£€æµ‹æ—¶é—´åå¥½
        if 'timestamp' in annotations_df.columns:
            annotations_df['hour'] = pd.to_datetime(annotations_df['timestamp']).dt.hour
            time_patterns = annotations_df.groupby('annotator')['hour'].apply(
                lambda x: x.value_counts(normalize=True).head(3)
            )
            patterns['time_preferences'] = time_patterns.to_dict()
        
        return patterns

# ä½¿ç”¨ç¤ºä¾‹
annotations_data = pd.DataFrame({
    'item_id': range(100),
    'annotator': np.random.choice(['A', 'B', 'C'], 100),
    'label': np.random.choice(['positive', 'negative', 'neutral'], 100),
    'confidence': np.random.uniform(0.5, 1.0, 100),
    'start_time': pd.date_range('2024-01-01', periods=100, freq='H'),
    'end_time': pd.date_range('2024-01-01 00:30:00', periods=100, freq='H')
})

qc = AnnotationQualityControl()
agreements = qc.inter_annotator_agreement(annotations_data)
time_stats = qc.calculate_annotation_time_stats(annotations_data)
patterns = qc.detect_annotation_patterns(annotations_data)

print("æ ‡æ³¨è€…é—´ä¸€è‡´æ€§:", agreements)
print("æ ‡æ³¨æ—¶é—´ç»Ÿè®¡:", time_stats)
print("æ ‡æ³¨æ¨¡å¼:", patterns)
```

### ä¸»åŠ¨å­¦ä¹ ä¸è‡ªåŠ¨åŒ–æ ‡æ³¨
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
import numpy as np

class ActiveLearningAnnotation:
    def __init__(self, model=None):
        self.model = model or RandomForestClassifier(n_estimators=100)
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.labeled_data = []
        self.unlabeled_data = []
    
    def add_labeled_data(self, texts, labels):
        """æ·»åŠ å·²æ ‡æ³¨æ•°æ®"""
        for text, label in zip(texts, labels):
            self.labeled_data.append({'text': text, 'label': label})
    
    def add_unlabeled_data(self, texts):
        """æ·»åŠ æœªæ ‡æ³¨æ•°æ®"""
        for text in texts:
            self.unlabeled_data.append({'text': text})
    
    def train_model(self):
        """è®­ç»ƒå½“å‰æ¨¡å‹"""
        if len(self.labeled_data) < 2:
            return False
        
        texts = [item['text'] for item in self.labeled_data]
        labels = [item['label'] for item in self.labeled_data]
        
        # ç‰¹å¾æå–
        X = self.vectorizer.fit_transform(texts)
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(X, labels)
        return True
    
    def uncertainty_sampling(self, n_samples=10):
        """ä¸ç¡®å®šæ€§é‡‡æ ·"""
        if not self.unlabeled_data:
            return []
        
        texts = [item['text'] for item in self.unlabeled_data]
        X = self.vectorizer.transform(texts)
        
        # è·å–é¢„æµ‹æ¦‚ç‡
        probabilities = self.model.predict_proba(X)
        
        # è®¡ç®—ä¸ç¡®å®šæ€§ï¼ˆç†µï¼‰
        uncertainties = []
        for prob in probabilities:
            entropy = -np.sum(prob * np.log(prob + 1e-10))
            uncertainties.append(entropy)
        
        # é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬
        uncertain_indices = np.argsort(uncertainties)[-n_samples:]
        
        selected_samples = []
        for idx in uncertain_indices:
            selected_samples.append({
                'index': idx,
                'text': self.unlabeled_data[idx]['text'],
                'uncertainty': uncertainties[idx],
                'probabilities': probabilities[idx].tolist()
            })
        
        return selected_samples
    
    def diversity_sampling(self, n_samples=10):
        """å¤šæ ·æ€§é‡‡æ ·"""
        if not self.unlabeled_data:
            return []
        
        texts = [item['text'] for item in self.unlabeled_data]
        X = self.vectorizer.transform(texts).toarray()
        
        # é€‰æ‹©å¤šæ ·æ€§æœ€å¤§çš„æ ·æœ¬
        selected_indices = []
        remaining_indices = list(range(len(texts)))
        
        # éšæœºé€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬
        first_idx = np.random.choice(remaining_indices)
        selected_indices.append(first_idx)
        remaining_indices.remove(first_idx)
        
        # è¿­ä»£é€‰æ‹©ä¸å·²é€‰æ ·æœ¬å·®å¼‚æœ€å¤§çš„æ ·æœ¬
        for _ in range(min(n_samples - 1, len(remaining_indices))):
            max_min_distance = -1
            best_idx = None
            
            for candidate_idx in remaining_indices:
                min_distance = float('inf')
                
                for selected_idx in selected_indices:
                    distance = np.linalg.norm(X[candidate_idx] - X[selected_idx])
                    min_distance = min(min_distance, distance)
                
                if min_distance > max_min_distance:
                    max_min_distance = min_distance
                    best_idx = candidate_idx
            
            if best_idx is not None:
                selected_indices.append(best_idx)
                remaining_indices.remove(best_idx)
        
        selected_samples = []
        for idx in selected_indices:
            selected_samples.append({
                'index': idx,
                'text': self.unlabeled_data[idx]['text']
            })
        
        return selected_samples
    
    def auto_annotate(self, confidence_threshold=0.9):
        """è‡ªåŠ¨æ ‡æ³¨é«˜ç½®ä¿¡åº¦æ ·æœ¬"""
        if not self.unlabeled_data:
            return []
        
        texts = [item['text'] for item in self.unlabeled_data]
        X = self.vectorizer.transform(texts)
        
        probabilities = self.model.predict_proba(X)
        predictions = self.model.predict(X)
        
        auto_annotated = []
        indices_to_remove = []
        
        for i, (prob, pred) in enumerate(zip(probabilities, predictions)):
            max_confidence = np.max(prob)
            
            if max_confidence >= confidence_threshold:
                auto_annotated.append({
                    'text': texts[i],
                    'label': pred,
                    'confidence': max_confidence
                })
                indices_to_remove.append(i)
        
        # ç§»é™¤è‡ªåŠ¨æ ‡æ³¨çš„æ ·æœ¬
        for idx in sorted(indices_to_remove, reverse=True):
            self.unlabeled_data.pop(idx)
        
        return auto_annotated

# ä½¿ç”¨ç¤ºä¾‹
# åˆå§‹åŒ–ä¸»åŠ¨å­¦ä¹ ç³»ç»Ÿ
al_system = ActiveLearningAnnotation()

# æ·»åŠ åˆå§‹æ ‡æ³¨æ•°æ®
initial_texts = ["è¿™ä¸ªäº§å“å¾ˆå¥½", "è´¨é‡å¤ªå·®äº†", "è¿˜å¯ä»¥å§"]
initial_labels = ["positive", "negative", "neutral"]
al_system.add_labeled_data(initial_texts, initial_labels)

# æ·»åŠ å¾…æ ‡æ³¨æ•°æ®
unlabeled_texts = ["éå¸¸æ»¡æ„", "ä¸æ¨èè´­ä¹°", "æ€§ä»·æ¯”ä¸€èˆ¬", "è¶…å‡ºé¢„æœŸ"]
al_system.add_unlabeled_data(unlabeled_texts)

# è®­ç»ƒåˆå§‹æ¨¡å‹
al_system.train_model()

# è·å–éœ€è¦äººå·¥æ ‡æ³¨çš„æ ·æœ¬
uncertain_samples = al_system.uncertainty_sampling(n_samples=2)
diverse_samples = al_system.diversity_sampling(n_samples=2)

print("ä¸ç¡®å®šæ€§é‡‡æ ·ç»“æœ:", uncertain_samples)
print("å¤šæ ·æ€§é‡‡æ ·ç»“æœ:", diverse_samples)

# è‡ªåŠ¨æ ‡æ³¨é«˜ç½®ä¿¡åº¦æ ·æœ¬
auto_labeled = al_system.auto_annotate(confidence_threshold=0.8)
print("è‡ªåŠ¨æ ‡æ³¨ç»“æœ:", auto_labeled)
```

## ğŸ¯ ç‰¹å®šé¢†åŸŸæ ‡æ³¨

### åŒ»å­¦å›¾åƒæ ‡æ³¨
```python
import cv2
import numpy as np
from sklearn.cluster import KMeans

class MedicalImageAnnotation:
    def __init__(self):
        self.annotations = {}
    
    def segment_lesion(self, image_path, roi_coordinates=None):
        """ç—…ç¶åˆ†å‰²æ ‡æ³¨"""
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        
        if roi_coordinates:
            # æå–æ„Ÿå…´è¶£åŒºåŸŸ
            x1, y1, x2, y2 = roi_coordinates
            roi = image[y1:y2, x1:x2]
        else:
            roi = image
        
        # ä½¿ç”¨K-meansè¿›è¡Œåˆæ­¥åˆ†å‰²
        data = roi.reshape((-1, 1))
        kmeans = KMeans(n_clusters=3, random_state=42)
        labels = kmeans.fit_predict(data)
        
        # é‡å¡‘ä¸ºå›¾åƒå½¢çŠ¶
        segmented = labels.reshape(roi.shape)
        
        return segmented, roi
    
    def annotate_anatomical_structure(self, image_path, structure_type):
        """è§£å‰–ç»“æ„æ ‡æ³¨"""
        structures = {
            'heart': {'color': (255, 0, 0), 'thickness': 2},
            'lung': {'color': (0, 255, 0), 'thickness': 2},
            'liver': {'color': (0, 0, 255), 'thickness': 2},
            'kidney': {'color': (255, 255, 0), 'thickness': 2}
        }
        
        annotation = {
            'image_path': image_path,
            'structure_type': structure_type,
            'properties': structures.get(structure_type, {'color': (255, 255, 255), 'thickness': 1}),
            'timestamp': datetime.now().isoformat()
        }
        
        return annotation
    
    def measure_distance(self, point1, point2, pixel_spacing):
        """æµ‹é‡è·ç¦»ï¼ˆæ¯«ç±³ï¼‰"""
        pixel_distance = np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)
        real_distance = pixel_distance * pixel_spacing
        return real_distance
    
    def calculate_area(self, contour, pixel_spacing):
        """è®¡ç®—é¢ç§¯ï¼ˆå¹³æ–¹æ¯«ç±³ï¼‰"""
        pixel_area = cv2.contourArea(contour)
        real_area = pixel_area * (pixel_spacing ** 2)
        return real_area

# æ³•å¾‹æ–‡æ¡£æ ‡æ³¨
class LegalDocumentAnnotation:
    def __init__(self):
        self.legal_entities = [
            'PERSON', 'ORGANIZATION', 'LOCATION', 'DATE', 'MONEY',
            'LAW', 'CASE', 'COURT', 'JUDGE', 'LAWYER'
        ]
    
    def extract_legal_entities(self, text):
        """æå–æ³•å¾‹å®ä½“"""
        # è¿™é‡Œå¯ä»¥é›†æˆä¸“é—¨çš„æ³•å¾‹NLPæ¨¡å‹
        entities = []
        
        # ç®€å•çš„è§„åˆ™åŸºç¡€å®ä½“è¯†åˆ«ç¤ºä¾‹
        import re
        
        # æ—¥æœŸæ¨¡å¼
        date_pattern = r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥'
        dates = re.finditer(date_pattern, text)
        for match in dates:
            entities.append({
                'text': match.group(),
                'type': 'DATE',
                'start': match.start(),
                'end': match.end()
            })
        
        # é‡‘é¢æ¨¡å¼
        money_pattern = r'\d+(?:,\d{3})*(?:\.\d{2})?å…ƒ'
        money_matches = re.finditer(money_pattern, text)
        for match in money_matches:
            entities.append({
                'text': match.group(),
                'type': 'MONEY',
                'start': match.start(),
                'end': match.end()
            })
        
        return entities
    
    def annotate_clause_type(self, clause_text):
        """æ ‡æ³¨æ¡æ¬¾ç±»å‹"""
        clause_types = {
            'æƒåˆ©ä¹‰åŠ¡': ['æƒåˆ©', 'ä¹‰åŠ¡', 'è´£ä»»', 'æƒé™'],
            'è¿çº¦è´£ä»»': ['è¿çº¦', 'èµ”å¿', 'æŸå¤±', 'è´£ä»»'],
            'äº‰è®®è§£å†³': ['äº‰è®®', 'ä»²è£', 'è¯‰è®¼', 'ç®¡è¾–'],
            'ç”Ÿæ•ˆæ¡ä»¶': ['ç”Ÿæ•ˆ', 'ç»ˆæ­¢', 'æœŸé™', 'æ¡ä»¶']
        }
        
        scores = {}
        for clause_type, keywords in clause_types.items():
            score = sum(1 for keyword in keywords if keyword in clause_text)
            scores[clause_type] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„æ¡æ¬¾ç±»å‹
        best_type = max(scores, key=scores.get)
        return best_type if scores[best_type] > 0 else 'å…¶ä»–'
```

## ğŸ“ˆ æ ‡æ³¨æ•ˆç‡ä¼˜åŒ–

### æ‰¹é‡æ ‡æ³¨ç­–ç•¥
```python
class BatchAnnotationStrategy:
    def __init__(self):
        self.batch_size = 50
        self.strategies = ['random', 'similar', 'diverse', 'uncertain']
    
    def create_batches(self, data, strategy='similar'):
        """åˆ›å»ºæ ‡æ³¨æ‰¹æ¬¡"""
        if strategy == 'random':
            return self._random_batches(data)
        elif strategy == 'similar':
            return self._similar_batches(data)
        elif strategy == 'diverse':
            return self._diverse_batches(data)
        elif strategy == 'uncertain':
            return self._uncertain_batches(data)
    
    def _similar_batches(self, data):
        """åˆ›å»ºç›¸ä¼¼æ ·æœ¬æ‰¹æ¬¡"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.cluster import KMeans
        
        # å‡è®¾dataæ˜¯æ–‡æœ¬åˆ—è¡¨
        vectorizer = TfidfVectorizer(max_features=100)
        features = vectorizer.fit_transform(data)
        
        # èšç±»
        n_clusters = len(data) // self.batch_size
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(features)
        
        # æŒ‰èšç±»åˆ†ç»„
        batches = {}
        for i, cluster_id in enumerate(clusters):
            if cluster_id not in batches:
                batches[cluster_id] = []
            batches[cluster_id].append(data[i])
        
        return list(batches.values())
    
    def estimate_annotation_time(self, batch, complexity_factor=1.0):
        """ä¼°è®¡æ ‡æ³¨æ—¶é—´"""
        base_time_per_sample = 30  # ç§’
        estimated_time = len(batch) * base_time_per_sample * complexity_factor
        return estimated_time
    
    def optimize_annotator_assignment(self, batches, annotators):
        """ä¼˜åŒ–æ ‡æ³¨è€…åˆ†é…"""
        assignments = {}
        
        for i, batch in enumerate(batches):
            # ç®€å•çš„è½®è¯¢åˆ†é…
            annotator = annotators[i % len(annotators)]
            if annotator not in assignments:
                assignments[annotator] = []
            assignments[annotator].append({
                'batch_id': i,
                'batch': batch,
                'estimated_time': self.estimate_annotation_time(batch)
            })
        
        return assignments
```

## ğŸ“š æœ€ä½³å®è·µ

### æ ‡æ³¨æŒ‡å—åˆ¶å®š
1. **æ˜ç¡®æ ‡æ³¨æ ‡å‡†**ï¼šè¯¦ç»†å®šä¹‰æ¯ä¸ªæ ‡ç­¾çš„å«ä¹‰å’Œé€‚ç”¨èŒƒå›´
2. **æä¾›ç¤ºä¾‹**ï¼šä¸ºæ¯ç§æƒ…å†µæä¾›æ­£é¢å’Œè´Ÿé¢ç¤ºä¾‹
3. **å¤„ç†è¾¹ç•Œæƒ…å†µ**ï¼šæ˜ç¡®æ¨¡ç³Šæƒ…å†µçš„å¤„ç†æ–¹å¼
4. **å»ºç«‹å®¡æ ¸æµç¨‹**ï¼šè®¾ç½®å¤šçº§å®¡æ ¸å’Œè´¨é‡æ§åˆ¶æœºåˆ¶

### æ ‡æ³¨è€…åŸ¹è®­
1. **ç†è®ºåŸ¹è®­**ï¼šä»‹ç»é¡¹ç›®èƒŒæ™¯å’Œæ ‡æ³¨ç›®æ ‡
2. **å®è·µè®­ç»ƒ**ï¼šé€šè¿‡ç¤ºä¾‹æ•°æ®è¿›è¡Œç»ƒä¹ 
3. **ä¸€è‡´æ€§æµ‹è¯•**ï¼šè¯„ä¼°æ ‡æ³¨è€…é—´çš„ä¸€è‡´æ€§
4. **æŒç»­åé¦ˆ**ï¼šå®šæœŸæ£€æŸ¥å’Œæ”¹è¿›æ ‡æ³¨è´¨é‡

### æŠ€æœ¯å·¥å…·é›†æˆ
1. **ç‰ˆæœ¬æ§åˆ¶**ï¼šä½¿ç”¨Gitç­‰å·¥å…·ç®¡ç†æ ‡æ³¨æ•°æ®ç‰ˆæœ¬
2. **è‡ªåŠ¨åŒ–æ£€æŸ¥**ï¼šå¼€å‘è„šæœ¬è‡ªåŠ¨æ£€æµ‹æ ‡æ³¨é”™è¯¯
3. **æ•°æ®å¤‡ä»½**ï¼šå®šæœŸå¤‡ä»½æ ‡æ³¨æ•°æ®é˜²æ­¢ä¸¢å¤±
4. **è¿›åº¦è·Ÿè¸ª**ï¼šå®æ—¶ç›‘æ§æ ‡æ³¨è¿›åº¦å’Œè´¨é‡æŒ‡æ ‡

## ğŸ”–æ€»ç»“
æ•°æ®æ ‡æ³¨æ˜¯è¿æ¥åŸå§‹æ•°æ®ä¸æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¡¥æ¢ï¼Œå…¶è´¨é‡å’Œæ•ˆç‡ç›´æ¥å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚éšç€æŠ€æœ¯çš„å‘å±•ï¼Œæ•°æ®æ ‡æ³¨æ–¹æ³•ä¹Ÿåœ¨ä¸æ–­åˆ›æ–°ï¼Œä»ä¼ ç»Ÿçš„äººå·¥æ ‡æ³¨åˆ°åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è‡ªåŠ¨æ ‡æ³¨ï¼Œé™ä½äº†æˆæœ¬ï¼Œæé«˜äº†ç²¾åº¦ã€‚

- **æœªç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ **ä¸ºå‡å°‘æ ‡æ³¨ä¾èµ–æä¾›äº†æ–°çš„æ€è·¯ï¼Œé€šè¿‡æŒ–æ˜æ•°æ®å†…åœ¨çš„ç»“æ„å’Œä¿¡æ¯ï¼Œè®­ç»ƒå‡ºå…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹ã€‚
- **ä¸°å¯Œçš„æ ‡æ³¨ç±»å‹**æ»¡è¶³äº†ä¸åŒåº”ç”¨åœºæ™¯çš„éœ€æ±‚ï¼Œä»ç®€å•çš„ç±»åˆ«æ ‡æ³¨åˆ°å¤æ‚çš„åœºæ™¯æµå’Œå ç”¨åœ°å›¾ï¼Œä¸ºæ¨¡å‹æä¾›äº†å¤šå±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚
- **å¤šæ ·åŒ–çš„æ ‡æ³¨æ–¹æ³•**ç»“åˆäº†äººç±»æ™ºèƒ½å’Œæœºå™¨æ™ºèƒ½ï¼Œä¸»åŠ¨å­¦ä¹ ã€ä¼—åŒ…æ ‡æ³¨ã€åˆæˆæ•°æ®ç­‰æ–¹æ³•ï¼Œä½¿å¾—æ•°æ®æ ‡æ³¨æ›´åŠ é«˜æ•ˆå’Œç»æµã€‚

åœ¨æœªæ¥çš„å‘å±•ä¸­ï¼Œæ•°æ®æ ‡æ³¨å°†ç»§ç»­æœç€**è‡ªåŠ¨åŒ–ã€æ™ºèƒ½åŒ–ã€é«˜è´¨é‡**çš„æ–¹å‘è¿ˆè¿›ï¼Œä¸ºäººå·¥æ™ºèƒ½çš„åº”ç”¨æä¾›æ›´åšå®çš„åŸºç¡€ã€‚

## ğŸ”— å¯¼èˆªé“¾æ¥

- [è¿”å›ä¸»é¡µ](../index.html)
- [ä¸Šä¸€æ¨¡å—ï¼šæ•°æ®æå–ä¸åˆ†æ](data-extraction.html)
- [ä¸‹ä¸€æ¨¡å—ï¼šæ•°æ®åœºæ™¯æ£€ç´¢](scene-retrieval.html)
