---
layout: page
title: æ•°æ®åœºæ™¯æ£€ç´¢
---

# æ•°æ®åœºæ™¯æ£€ç´¢

> ğŸ” **æ¨¡å—ç›®æ ‡**ï¼šæŒæ¡å¤šæ¨¡æ€æ•°æ®åœºæ™¯æ£€ç´¢æŠ€æœ¯ï¼Œå®ç°æ™ºèƒ½åœºæ™¯åŒ¹é…å’Œå†…å®¹å‘ç°

## ğŸŒŸ åœºæ™¯æ£€ç´¢æ¦‚è¿°

æ•°æ®åœºæ™¯æ£€ç´¢æ˜¯æŒ‡æ ¹æ®æŸ¥è¯¢æ¡ä»¶ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­å¿«é€Ÿå‡†ç¡®åœ°æ‰¾åˆ°ç›¸å…³åœºæ™¯æˆ–å†…å®¹çš„æŠ€æœ¯ã€‚å®ƒç»“åˆäº†è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å¤šåª’ä½“æ£€ç´¢ç­‰å¤šä¸ªé¢†åŸŸçš„æŠ€æœ¯ï¼Œåœ¨æ™ºèƒ½æœç´¢ã€å†…å®¹æ¨èã€åœºæ™¯ç†è§£ç­‰åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ—ï¸ æ£€ç´¢ç³»ç»Ÿæ¶æ„

### ç³»ç»Ÿç»„ä»¶
```python
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import cv2
import torch
from sentence_transformers import SentenceTransformer
import faiss
import json
from datetime import datetime

class MultiModalRetrievalSystem:
    def __init__(self):
        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.image_features = {}
        self.text_features = {}
        self.metadata = {}
        self.indexes = {}
        
    def initialize_indexes(self):
        """åˆå§‹åŒ–å„ç§æ£€ç´¢ç´¢å¼•"""
        # æ–‡æœ¬ç´¢å¼•
        self.indexes['text'] = faiss.IndexFlatIP(384)  # 384ç»´åº¦
        
        # å›¾åƒç´¢å¼•
        self.indexes['image'] = faiss.IndexFlatIP(2048)  # ResNetç‰¹å¾ç»´åº¦
        
        # æ··åˆç´¢å¼•
        self.indexes['hybrid'] = faiss.IndexFlatIP(512)  # æ··åˆç‰¹å¾ç»´åº¦
    
    def add_data(self, data_id, text_content=None, image_path=None, metadata=None):
        """æ·»åŠ æ•°æ®åˆ°æ£€ç´¢ç³»ç»Ÿ"""
        features = {}
        
        # å¤„ç†æ–‡æœ¬ç‰¹å¾
        if text_content:
            text_embedding = self.text_encoder.encode([text_content])
            features['text'] = text_embedding[0]
            self.text_features[data_id] = text_embedding[0]
        
        # å¤„ç†å›¾åƒç‰¹å¾
        if image_path:
            image_features = self.extract_image_features(image_path)
            features['image'] = image_features
            self.image_features[data_id] = image_features
        
        # å­˜å‚¨å…ƒæ•°æ®
        if metadata:
            self.metadata[data_id] = metadata
        
        return features
    
    def extract_image_features(self, image_path):
        """æå–å›¾åƒç‰¹å¾"""
        # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNetæ¨¡å‹æå–ç‰¹å¾
        import torchvision.models as models
        import torchvision.transforms as transforms
        from PIL import Image
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        model = models.resnet50(pretrained=True)
        model.eval()
        
        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])
        
        # å›¾åƒé¢„å¤„ç†
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # åŠ è½½å’Œå¤„ç†å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        image_tensor = transform(image).unsqueeze(0)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            features = feature_extractor(image_tensor)
            features = features.squeeze().numpy()
        
        return features
    
    def build_indexes(self):
        """æ„å»ºæ‰€æœ‰ç´¢å¼•"""
        # æ„å»ºæ–‡æœ¬ç´¢å¼•
        if self.text_features:
            text_matrix = np.vstack(list(self.text_features.values()))
            self.indexes['text'].add(text_matrix.astype('float32'))
        
        # æ„å»ºå›¾åƒç´¢å¼•
        if self.image_features:
            image_matrix = np.vstack(list(self.image_features.values()))
            self.indexes['image'].add(image_matrix.astype('float32'))
    
    def search_text(self, query, k=10):
        """æ–‡æœ¬æ£€ç´¢"""
        query_embedding = self.text_encoder.encode([query])
        query_vector = query_embedding[0].astype('float32').reshape(1, -1)
        
        scores, indices = self.indexes['text'].search(query_vector, k)
        
        results = []
        data_ids = list(self.text_features.keys())
        
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(data_ids):
                data_id = data_ids[idx]
                results.append({
                    'data_id': data_id,
                    'score': float(score),
                    'metadata': self.metadata.get(data_id, {})
                })
        
        return results
    
    def search_image(self, image_path, k=10):
        """å›¾åƒæ£€ç´¢"""
        query_features = self.extract_image_features(image_path)
        query_vector = query_features.astype('float32').reshape(1, -1)
        
        scores, indices = self.indexes['image'].search(query_vector, k)
        
        results = []
        data_ids = list(self.image_features.keys())
        
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(data_ids):
                data_id = data_ids[idx]
                results.append({
                    'data_id': data_id,
                    'score': float(score),
                    'metadata': self.metadata.get(data_id, {})
                })
        
        return results
    
    def hybrid_search(self, text_query=None, image_path=None, text_weight=0.5, k=10):
        """æ··åˆæ£€ç´¢"""
        combined_scores = {}
        
        # æ–‡æœ¬æ£€ç´¢ç»“æœ
        if text_query:
            text_results = self.search_text(text_query, k*2)
            for result in text_results:
                data_id = result['data_id']
                combined_scores[data_id] = result['score'] * text_weight
        
        # å›¾åƒæ£€ç´¢ç»“æœ
        if image_path:
            image_results = self.search_image(image_path, k*2)
            for result in image_results:
                data_id = result['data_id']
                if data_id in combined_scores:
                    combined_scores[data_id] += result['score'] * (1 - text_weight)
                else:
                    combined_scores[data_id] = result['score'] * (1 - text_weight)
        
        # æ’åºå¹¶è¿”å›ç»“æœ
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        
        results = []
        for data_id, score in sorted_results[:k]:
            results.append({
                'data_id': data_id,
                'score': score,
                'metadata': self.metadata.get(data_id, {})
            })
        
        return results
```

### åœºæ™¯ç†è§£ä¸ç´¢å¼•
```python
class SceneUnderstandingModule:
    def __init__(self):
        self.scene_classifier = None
        self.object_detector = None
        self.caption_generator = None
        
    def analyze_scene(self, image_path):
        """å…¨é¢åœºæ™¯åˆ†æ"""
        analysis = {
            'scene_category': self.classify_scene(image_path),
            'objects': self.detect_objects(image_path),
            'spatial_layout': self.analyze_spatial_layout(image_path),
            'visual_attributes': self.extract_visual_attributes(image_path),
            'scene_caption': self.generate_caption(image_path)
        }
        return analysis
    
    def classify_scene(self, image_path):
        """åœºæ™¯åˆ†ç±»"""
        # ä½¿ç”¨é¢„è®­ç»ƒçš„åœºæ™¯åˆ†ç±»æ¨¡å‹
        categories = {
            'indoor': ['bedroom', 'kitchen', 'living_room', 'office', 'bathroom'],
            'outdoor': ['street', 'park', 'beach', 'mountain', 'urban'],
            'nature': ['forest', 'lake', 'desert', 'field', 'sky']
        }
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„åœºæ™¯åˆ†ç±»æ¨¡å‹
        # ç¤ºä¾‹è¿”å›
        return {
            'main_category': 'outdoor',
            'sub_category': 'street',
            'confidence': 0.85
        }
    
    def detect_objects(self, image_path):
        """ç‰©ä½“æ£€æµ‹"""
        # ä½¿ç”¨YOLOæˆ–å…¶ä»–ç‰©ä½“æ£€æµ‹æ¨¡å‹
        detected_objects = [
            {'class': 'car', 'confidence': 0.95, 'bbox': [100, 150, 300, 400]},
            {'class': 'person', 'confidence': 0.88, 'bbox': [50, 100, 150, 350]},
            {'class': 'tree', 'confidence': 0.75, 'bbox': [400, 50, 600, 300]}
        ]
        return detected_objects
    
    def analyze_spatial_layout(self, image_path):
        """ç©ºé—´å¸ƒå±€åˆ†æ"""
        layout = {
            'dominant_regions': ['sky', 'ground', 'buildings'],
            'perspective': 'street_level',
            'depth_information': {
                'foreground': ['person', 'car'],
                'background': ['buildings', 'sky']
            }
        }
        return layout
    
    def extract_visual_attributes(self, image_path):
        """æå–è§†è§‰å±æ€§"""
        attributes = {
            'color_palette': ['blue', 'gray', 'green'],
            'lighting': 'daylight',
            'weather': 'clear',
            'season': 'summer',
            'time_of_day': 'afternoon'
        }
        return attributes
    
    def generate_caption(self, image_path):
        """ç”Ÿæˆåœºæ™¯æè¿°"""
        # ä½¿ç”¨å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹
        caption = "A busy street scene with cars and pedestrians during daytime"
        return caption
    
    def create_scene_index(self, scene_analysis):
        """åˆ›å»ºåœºæ™¯æ£€ç´¢ç´¢å¼•"""
        index_data = {
            'scene_tags': [
                scene_analysis['scene_category']['main_category'],
                scene_analysis['scene_category']['sub_category']
            ],
            'object_tags': [obj['class'] for obj in scene_analysis['objects']],
            'attribute_tags': [
                scene_analysis['visual_attributes']['lighting'],
                scene_analysis['visual_attributes']['weather'],
                scene_analysis['visual_attributes']['time_of_day']
            ],
            'text_description': scene_analysis['scene_caption']
        }
        return index_data
```

## ğŸ” é«˜çº§æ£€ç´¢æŠ€æœ¯

### è¯­ä¹‰æ£€ç´¢
```python
from transformers import CLIPModel, CLIPProcessor
import torch

class SemanticRetrievalEngine:
    def __init__(self):
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.clip_model.to(self.device)
        
    def encode_text(self, text_list):
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
        inputs = self.clip_processor(text=text_list, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            text_features = self.clip_model.get_text_features(**inputs)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        return text_features.cpu().numpy()
    
    def encode_image(self, image_list):
        """ç¼–ç å›¾åƒä¸ºå‘é‡"""
        inputs = self.clip_processor(images=image_list, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        return image_features.cpu().numpy()
    
    def compute_similarity(self, text_features, image_features):
        """è®¡ç®—æ–‡æœ¬-å›¾åƒç›¸ä¼¼åº¦"""
        similarity = torch.matmul(
            torch.tensor(text_features), 
            torch.tensor(image_features).T
        )
        return similarity.numpy()
    
    def cross_modal_search(self, query_text, image_database, top_k=10):
        """è·¨æ¨¡æ€æ£€ç´¢"""
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        text_features = self.encode_text([query_text])
        
        # ç¼–ç å›¾åƒæ•°æ®åº“
        image_features = self.encode_image(image_database)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = self.compute_similarity(text_features, image_features)
        
        # è·å–top-kç»“æœ
        top_indices = np.argsort(similarities[0])[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'image_index': idx,
                'similarity_score': similarities[0][idx],
                'image_path': image_database[idx] if isinstance(image_database[idx], str) else None
            })
        
        return results
```

### åŸºäºå›¾ç»“æ„çš„æ£€ç´¢
```python
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer

class GraphBasedRetrieval:
    def __init__(self):
        self.knowledge_graph = nx.Graph()
        self.entity_embeddings = {}
        self.relation_types = ['similar_to', 'contains', 'located_in', 'related_to']
        
    def build_knowledge_graph(self, entities, relations):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        # æ·»åŠ å®ä½“èŠ‚ç‚¹
        for entity in entities:
            self.knowledge_graph.add_node(
                entity['id'], 
                type=entity['type'],
                attributes=entity.get('attributes', {})
            )
        
        # æ·»åŠ å…³ç³»è¾¹
        for relation in relations:
            self.knowledge_graph.add_edge(
                relation['source'],
                relation['target'],
                relation_type=relation['type'],
                weight=relation.get('weight', 1.0)
            )
    
    def entity_expansion(self, query_entities, max_hops=2):
        """å®ä½“æ‰©å±•"""
        expanded_entities = set(query_entities)
        
        for hop in range(max_hops):
            current_entities = expanded_entities.copy()
            for entity in current_entities:
                if entity in self.knowledge_graph:
                    neighbors = list(self.knowledge_graph.neighbors(entity))
                    expanded_entities.update(neighbors)
        
        return list(expanded_entities)
    
    def graph_based_ranking(self, query_entities, candidate_entities):
        """åŸºäºå›¾çš„æ’åº"""
        scores = {}
        
        for candidate in candidate_entities:
            score = 0
            
            for query_entity in query_entities:
                if query_entity in self.knowledge_graph and candidate in self.knowledge_graph:
                    try:
                        # è®¡ç®—æœ€çŸ­è·¯å¾„é•¿åº¦
                        path_length = nx.shortest_path_length(
                            self.knowledge_graph, query_entity, candidate
                        )
                        # è·ç¦»è¶Šè¿‘åˆ†æ•°è¶Šé«˜
                        score += 1.0 / (1 + path_length)
                    except nx.NetworkXNoPath:
                        # æ²¡æœ‰è·¯å¾„è¿æ¥
                        continue
            
            scores[candidate] = score
        
        # æŒ‰åˆ†æ•°æ’åº
        ranked_entities = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return ranked_entities
    
    def personalized_pagerank(self, query_entities, alpha=0.15):
        """ä¸ªæ€§åŒ–PageRank"""
        # åˆ›å»ºä¸ªæ€§åŒ–å‘é‡
        personalization = {}
        for node in self.knowledge_graph.nodes():
            if node in query_entities:
                personalization[node] = 1.0 / len(query_entities)
            else:
                personalization[node] = 0.0
        
        # è®¡ç®—ä¸ªæ€§åŒ–PageRank
        pagerank_scores = nx.pagerank(
            self.knowledge_graph,
            personalization=personalization,
            alpha=alpha
        )
        
        return pagerank_scores
```

### æ·±åº¦å­¦ä¹ æ£€ç´¢
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DeepRetrievalModel(nn.Module):
    def __init__(self, text_dim=768, image_dim=2048, hidden_dim=512):
        super(DeepRetrievalModel, self).__init__()
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = nn.Sequential(
            nn.Linear(text_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # å›¾åƒç¼–ç å™¨
        self.image_encoder = nn.Sequential(
            nn.Linear(image_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # äº¤äº’å±‚
        self.interaction_layer = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            batch_first=True
        )
        
        # ç›¸ä¼¼åº¦é¢„æµ‹å±‚
        self.similarity_predictor = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
    
    def forward(self, text_features, image_features):
        # ç¼–ç 
        text_encoded = self.text_encoder(text_features)
        image_encoded = self.image_encoder(image_features)
        
        # äº¤äº’æ³¨æ„åŠ›
        text_attended, _ = self.interaction_layer(
            text_encoded.unsqueeze(1),
            image_encoded.unsqueeze(1),
            image_encoded.unsqueeze(1)
        )
        text_attended = text_attended.squeeze(1)
        
        image_attended, _ = self.interaction_layer(
            image_encoded.unsqueeze(1),
            text_encoded.unsqueeze(1),
            text_encoded.unsqueeze(1)
        )
        image_attended = image_attended.squeeze(1)
        
        # èåˆç‰¹å¾
        fused_features = torch.cat([text_attended, image_attended], dim=-1)
        
        # é¢„æµ‹ç›¸ä¼¼åº¦
        similarity = self.similarity_predictor(fused_features)
        
        return similarity

class DeepRetrievalTrainer:
    def __init__(self, model, learning_rate=0.001):
        self.model = model
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        self.criterion = nn.BCELoss()
        
    def train_step(self, text_features, image_features, labels):
        self.optimizer.zero_grad()
        
        predictions = self.model(text_features, image_features)
        loss = self.criterion(predictions.squeeze(), labels.float())
        
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def evaluate(self, test_loader):
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for text_features, image_features, labels in test_loader:
                predictions = self.model(text_features, image_features)
                loss = self.criterion(predictions.squeeze(), labels.float())
                
                total_loss += loss.item()
                predicted = (predictions.squeeze() > 0.5).long()
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        avg_loss = total_loss / len(test_loader)
        
        return avg_loss, accuracy
```

## ğŸ¯ ç‰¹å®šåœºæ™¯åº”ç”¨

### æ™ºèƒ½ç›‘æ§æ£€ç´¢
```python
class SecurityRetrievalSystem:
    def __init__(self):
        self.alert_patterns = {
            'suspicious_behavior': ['loitering', 'running', 'fighting'],
            'object_detection': ['weapon', 'bag', 'vehicle'],
            'crowd_analysis': ['gathering', 'dispersal', 'density']
        }
        
    def analyze_security_footage(self, video_path, time_range=None):
        """åˆ†æå®‰é˜²è§†é¢‘"""
        analysis_results = []
        
        # è§†é¢‘å¸§æå–å’Œåˆ†æ
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        
        while cap.read()[0]:
            ret, frame = cap.read()
            if not ret:
                break
                
            frame_count += 1
            
            # æ¯éš”30å¸§åˆ†æä¸€æ¬¡ï¼ˆçº¦1ç§’ï¼‰
            if frame_count % 30 == 0:
                timestamp = frame_count / 30
                
                # è¡Œä¸ºåˆ†æ
                behaviors = self.detect_behaviors(frame)
                
                # ç‰©ä½“æ£€æµ‹
                objects = self.detect_suspicious_objects(frame)
                
                # äººç¾¤åˆ†æ
                crowd_info = self.analyze_crowd(frame)
                
                analysis_results.append({
                    'timestamp': timestamp,
                    'behaviors': behaviors,
                    'objects': objects,
                    'crowd_info': crowd_info,
                    'alert_level': self.calculate_alert_level(behaviors, objects, crowd_info)
                })
        
        cap.release()
        return analysis_results
    
    def detect_behaviors(self, frame):
        """æ£€æµ‹å¯ç–‘è¡Œä¸º"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„è¡Œä¸ºè¯†åˆ«æ¨¡å‹
        behaviors = [
            {'type': 'walking', 'confidence': 0.9, 'person_id': 1},
            {'type': 'loitering', 'confidence': 0.7, 'person_id': 2}
        ]
        return behaviors
    
    def detect_suspicious_objects(self, frame):
        """æ£€æµ‹å¯ç–‘ç‰©ä½“"""
        objects = [
            {'type': 'bag', 'confidence': 0.8, 'bbox': [100, 100, 200, 200]},
            {'type': 'person', 'confidence': 0.95, 'bbox': [50, 50, 150, 300]}
        ]
        return objects
    
    def analyze_crowd(self, frame):
        """äººç¾¤åˆ†æ"""
        crowd_info = {
            'person_count': 15,
            'density': 'medium',
            'movement_direction': 'north',
            'activity_level': 'normal'
        }
        return crowd_info
    
    def calculate_alert_level(self, behaviors, objects, crowd_info):
        """è®¡ç®—è­¦æŠ¥çº§åˆ«"""
        alert_score = 0
        
        # è¡Œä¸ºè¯„åˆ†
        for behavior in behaviors:
            if behavior['type'] in self.alert_patterns['suspicious_behavior']:
                alert_score += behavior['confidence'] * 2
        
        # ç‰©ä½“è¯„åˆ†
        for obj in objects:
            if obj['type'] in self.alert_patterns['object_detection']:
                alert_score += obj['confidence'] * 3
        
        # äººç¾¤è¯„åˆ†
        if crowd_info['density'] == 'high':
            alert_score += 1
        
        # ç¡®å®šè­¦æŠ¥çº§åˆ«
        if alert_score > 3:
            return 'high'
        elif alert_score > 1.5:
            return 'medium'
        else:
            return 'low'
    
    def query_incidents(self, query_type, time_range, location=None):
        """æŸ¥è¯¢å†å²äº‹ä»¶"""
        # è¿™é‡Œåº”è¯¥è¿æ¥åˆ°äº‹ä»¶æ•°æ®åº“
        incidents = [
            {
                'timestamp': '2024-01-15 14:30:00',
                'type': 'suspicious_behavior',
                'location': 'entrance_gate',
                'alert_level': 'medium',
                'description': 'Person loitering near entrance for extended period'
            },
            {
                'timestamp': '2024-01-15 16:45:00',
                'type': 'object_detection',
                'location': 'parking_lot',
                'alert_level': 'high',
                'description': 'Unattended bag detected in parking area'
            }
        ]
        
        return incidents
```

### ç”µå•†åœºæ™¯æ£€ç´¢
```python
class EcommerceSceneRetrieval:
    def __init__(self):
        self.product_attributes = ['color', 'style', 'brand', 'category', 'price_range']
        self.scene_contexts = ['indoor', 'outdoor', 'casual', 'formal', 'seasonal']
        
    def visual_search(self, query_image_path, filters=None):
        """è§†è§‰æœç´¢å•†å“"""
        # æå–æŸ¥è¯¢å›¾åƒç‰¹å¾
        query_features = self.extract_product_features(query_image_path)
        
        # åœºæ™¯ç†è§£
        scene_context = self.understand_scene_context(query_image_path)
        
        # å•†å“æ£€ç´¢
        similar_products = self.find_similar_products(query_features, scene_context, filters)
        
        return similar_products
    
    def extract_product_features(self, image_path):
        """æå–å•†å“ç‰¹å¾"""
        features = {
            'color_palette': ['red', 'black', 'white'],
            'style_tags': ['casual', 'modern'],
            'category': 'clothing',
            'visual_features': np.random.rand(512)  # å®é™…åº”è¯¥æ˜¯æ·±åº¦å­¦ä¹ ç‰¹å¾
        }
        return features
    
    def understand_scene_context(self, image_path):
        """ç†è§£åœºæ™¯ä¸Šä¸‹æ–‡"""
        context = {
            'setting': 'outdoor',
            'occasion': 'casual',
            'season': 'summer',
            'weather': 'sunny'
        }
        return context
    
    def find_similar_products(self, query_features, scene_context, filters=None):
        """æŸ¥æ‰¾ç›¸ä¼¼å•†å“"""
        # æ¨¡æ‹Ÿå•†å“æ•°æ®åº“æŸ¥è¯¢
        products = [
            {
                'product_id': 'P001',
                'name': 'Summer Casual T-Shirt',
                'category': 'clothing',
                'price': 29.99,
                'colors': ['red', 'blue', 'white'],
                'style_tags': ['casual', 'summer'],
                'similarity_score': 0.95
            },
            {
                'product_id': 'P002',
                'name': 'Outdoor Adventure Jacket',
                'category': 'clothing',
                'price': 89.99,
                'colors': ['black', 'green'],
                'style_tags': ['outdoor', 'adventure'],
                'similarity_score': 0.82
            }
        ]
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if filters:
            filtered_products = []
            for product in products:
                if self.apply_filters(product, filters):
                    filtered_products.append(product)
            products = filtered_products
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        products.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        return products
    
    def apply_filters(self, product, filters):
        """åº”ç”¨æœç´¢è¿‡æ»¤æ¡ä»¶"""
        for filter_type, filter_value in filters.items():
            if filter_type == 'price_range':
                min_price, max_price = filter_value
                if not (min_price <= product['price'] <= max_price):
                    return False
            elif filter_type == 'category':
                if product['category'] != filter_value:
                    return False
            elif filter_type == 'color':
                if filter_value not in product['colors']:
                    return False
        
        return True
    
    def recommendation_by_scene(self, user_profile, current_scene):
        """åŸºäºåœºæ™¯çš„å•†å“æ¨è"""
        recommendations = []
        
        # æ ¹æ®åœºæ™¯æ¨è
        if current_scene['setting'] == 'outdoor':
            if current_scene['season'] == 'summer':
                recommendations.extend(['sunglasses', 'hat', 'sunscreen'])
            elif current_scene['season'] == 'winter':
                recommendations.extend(['jacket', 'gloves', 'boots'])
        
        # æ ¹æ®ç”¨æˆ·å†å²åå¥½è°ƒæ•´
        if user_profile.get('preferred_brands'):
            # è¿‡æ»¤æ¨èå“ç‰Œ
            pass
        
        return recommendations
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### ç´¢å¼•ä¼˜åŒ–
```python
class IndexOptimizer:
    def __init__(self):
        self.index_types = ['flat', 'ivf', 'hnsw', 'pq']
        
    def benchmark_indexes(self, data, queries, index_configs):
        """åŸºå‡†æµ‹è¯•ä¸åŒç´¢å¼•"""
        results = {}
        
        for config in index_configs:
            index_type = config['type']
            params = config.get('params', {})
            
            # æ„å»ºç´¢å¼•
            start_time = time.time()
            index = self.build_index(data, index_type, params)
            build_time = time.time() - start_time
            
            # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
            start_time = time.time()
            search_results = []
            for query in queries:
                results_per_query = index.search(query, k=10)
                search_results.append(results_per_query)
            search_time = time.time() - start_time
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = self.calculate_accuracy(search_results, ground_truth=None)
            
            results[index_type] = {
                'build_time': build_time,
                'search_time': search_time,
                'accuracy': accuracy,
                'memory_usage': self.get_memory_usage(index)
            }
        
        return results
    
    def build_index(self, data, index_type, params):
        """æ„å»ºæŒ‡å®šç±»å‹çš„ç´¢å¼•"""
        dim = data.shape[1]
        
        if index_type == 'flat':
            index = faiss.IndexFlatIP(dim)
        elif index_type == 'ivf':
            nlist = params.get('nlist', 100)
            quantizer = faiss.IndexFlatIP(dim)
            index = faiss.IndexIVFFlat(quantizer, dim, nlist)
        elif index_type == 'hnsw':
            m = params.get('m', 16)
            index = faiss.IndexHNSWFlat(dim, m)
        elif index_type == 'pq':
            m = params.get('m', 8)
            nbits = params.get('nbits', 8)
            index = faiss.IndexPQ(dim, m, nbits)
        
        index.add(data.astype('float32'))
        return index
    
    def optimize_query_processing(self, queries):
        """ä¼˜åŒ–æŸ¥è¯¢å¤„ç†"""
        # æŸ¥è¯¢ç¼“å­˜
        query_cache = {}
        
        # æ‰¹é‡å¤„ç†
        batch_size = 32
        batched_queries = [queries[i:i+batch_size] for i in range(0, len(queries), batch_size)]
        
        # æŸ¥è¯¢é‡å†™å’Œæ‰©å±•
        optimized_queries = []
        for query in queries:
            if query in query_cache:
                optimized_queries.append(query_cache[query])
            else:
                expanded_query = self.expand_query(query)
                query_cache[query] = expanded_query
                optimized_queries.append(expanded_query)
        
        return optimized_queries
    
    def expand_query(self, query):
        """æŸ¥è¯¢æ‰©å±•"""
        # åŒä¹‰è¯æ‰©å±•
        synonyms = self.get_synonyms(query)
        
        # ç›¸å…³è¯æ‰©å±•
        related_terms = self.get_related_terms(query)
        
        expanded_query = {
            'original': query,
            'synonyms': synonyms,
            'related_terms': related_terms
        }
        
        return expanded_query
```

### ç¼“å­˜ç­–ç•¥
```python
import redis
import pickle
from functools import wraps

class RetrievalCache:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=False)
        self.default_ttl = 3600  # 1å°æ—¶
        
    def cache_key(self, query, params=None):
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        key_data = f"{query}_{params}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get_cached_result(self, query, params=None):
        """è·å–ç¼“å­˜ç»“æœ"""
        key = self.cache_key(query, params)
        cached_data = self.redis_client.get(key)
        
        if cached_data:
            return pickle.loads(cached_data)
        return None
    
    def cache_result(self, query, result, params=None, ttl=None):
        """ç¼“å­˜ç»“æœ"""
        key = self.cache_key(query, params)
        ttl = ttl or self.default_ttl
        
        self.redis_client.setex(key, ttl, pickle.dumps(result))
    
    def cache_decorator(self, ttl=None):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = f"{func.__name__}_{str(args)}_{str(kwargs)}"
                
                # å°è¯•è·å–ç¼“å­˜
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return pickle.loads(cached_result)
                
                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = func(*args, **kwargs)
                self.redis_client.setex(
                    cache_key, 
                    ttl or self.default_ttl, 
                    pickle.dumps(result)
                )
                
                return result
            return wrapper
        return decorator

# ä½¿ç”¨ç¤ºä¾‹
cache = RetrievalCache()

@cache.cache_decorator(ttl=7200)  # ç¼“å­˜2å°æ—¶
def expensive_search_operation(query, filters):
    # æ‰§è¡Œè€—æ—¶çš„æœç´¢æ“ä½œ
    time.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return f"Result for {query} with filters {filters}"
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### æ£€ç´¢æ€§èƒ½è¯„ä¼°
```python
class RetrievalEvaluator:
    def __init__(self):
        pass
    
    def precision_at_k(self, retrieved_items, relevant_items, k):
        """è®¡ç®—P@K"""
        retrieved_k = retrieved_items[:k]
        relevant_retrieved = len(set(retrieved_k) & set(relevant_items))
        return relevant_retrieved / k if k > 0 else 0
    
    def recall_at_k(self, retrieved_items, relevant_items, k):
        """è®¡ç®—R@K"""
        retrieved_k = retrieved_items[:k]
        relevant_retrieved = len(set(retrieved_k) & set(relevant_items))
        return relevant_retrieved / len(relevant_items) if len(relevant_items) > 0 else 0
    
    def average_precision(self, retrieved_items, relevant_items):
        """è®¡ç®—å¹³å‡ç²¾åº¦"""
        if not relevant_items:
            return 0
        
        precision_sum = 0
        relevant_count = 0
        
        for i, item in enumerate(retrieved_items):
            if item in relevant_items:
                relevant_count += 1
                precision_sum += relevant_count / (i + 1)
        
        return precision_sum / len(relevant_items)
    
    def mean_average_precision(self, all_retrieved, all_relevant):
        """è®¡ç®—MAP"""
        ap_scores = []
        for retrieved, relevant in zip(all_retrieved, all_relevant):
            ap = self.average_precision(retrieved, relevant)
            ap_scores.append(ap)
        
        return np.mean(ap_scores)
    
    def normalized_dcg(self, retrieved_items, relevance_scores, k=None):
        """è®¡ç®—NDCG"""
        if k:
            retrieved_items = retrieved_items[:k]
            relevance_scores = relevance_scores[:k]
        
        # è®¡ç®—DCG
        dcg = 0
        for i, item in enumerate(retrieved_items):
            if item in relevance_scores:
                relevance = relevance_scores[item]
                dcg += (2**relevance - 1) / np.log2(i + 2)
        
        # è®¡ç®—IDCG
        ideal_relevance = sorted(relevance_scores.values(), reverse=True)
        idcg = 0
        for i, relevance in enumerate(ideal_relevance):
            idcg += (2**relevance - 1) / np.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0
    
    def evaluate_retrieval_system(self, test_queries, ground_truth, system_results):
        """ç»¼åˆè¯„ä¼°æ£€ç´¢ç³»ç»Ÿ"""
        metrics = {
            'precision_at_1': [],
            'precision_at_5': [],
            'precision_at_10': [],
            'recall_at_10': [],
            'map': [],
            'ndcg_at_10': []
        }
        
        for query_id, retrieved in system_results.items():
            if query_id in ground_truth:
                relevant = ground_truth[query_id]
                
                # è®¡ç®—å„ç§æŒ‡æ ‡
                metrics['precision_at_1'].append(
                    self.precision_at_k(retrieved, relevant, 1)
                )
                metrics['precision_at_5'].append(
                    self.precision_at_k(retrieved, relevant, 5)
                )
                metrics['precision_at_10'].append(
                    self.precision_at_k(retrieved, relevant, 10)
                )
                metrics['recall_at_10'].append(
                    self.recall_at_k(retrieved, relevant, 10)
                )
                metrics['map'].append(
                    self.average_precision(retrieved, relevant)
                )
        
        # è®¡ç®—å¹³å‡å€¼
        final_metrics = {}
        for metric_name, values in metrics.items():
            final_metrics[metric_name] = np.mean(values)
        
        return final_metrics
```

## ğŸ”— å¯¼èˆªé“¾æ¥

- [è¿”å›ä¸»é¡µ](../index.html)
- [ä¸Šä¸€æ¨¡å—ï¼šæ•°æ®æ ‡æ³¨](data-annotation.html)
- [ä¸‹ä¸€æ¨¡å—ï¼šæ•°æ®ä¸åœºæ™¯å¯è§†åŒ–](data-visualization.html)
