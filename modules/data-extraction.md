---
layout: page
title: æ•°æ®æå–ä¸åˆ†æ
---

# æ•°æ®æå–ä¸åˆ†æ

> ğŸ¯ **æ¨¡å—ç›®æ ‡**ï¼šå­¦ä¹ æ•°æ®æå–æŠ€æœ¯å’Œé«˜çº§åˆ†ææ–¹æ³•ï¼Œä»å¤šæºæ•°æ®ä¸­è·å–ä»·å€¼

## ğŸ” æ•°æ®æå–æ¦‚è¿°

æ•°æ®æå–æ˜¯æ•°æ®åˆ†ææµç¨‹çš„å…³é”®ç¬¬ä¸€æ­¥ï¼Œæ¶‰åŠä»å„ç§æ•°æ®æºä¸­è·å–ã€è½¬æ¢å’ŒåŠ è½½æ•°æ®ã€‚æœ¬æ¨¡å—å°†ä»‹ç»ç°ä»£æ•°æ®æå–æŠ€æœ¯å’Œåˆ†æç­–ç•¥ã€‚

## ğŸ“Š æ•°æ®æºç±»å‹

### ç»“æ„åŒ–æ•°æ®æº
- **å…³ç³»å‹æ•°æ®åº“**ï¼šMySQL, PostgreSQL, Oracle
- **NoSQLæ•°æ®åº“**ï¼šMongoDB, Cassandra, Redis
- **æ•°æ®ä»“åº“**ï¼šSnowflake, BigQuery, Redshift
- **æ–‡ä»¶ç³»ç»Ÿ**ï¼šCSV, Excel, Parquet, JSON

### éç»“æ„åŒ–æ•°æ®æº
- **æ–‡æœ¬æ•°æ®**ï¼šæ—¥å¿—æ–‡ä»¶ã€æ–‡æ¡£ã€ç¤¾äº¤åª’ä½“
- **å›¾åƒæ•°æ®**ï¼šç…§ç‰‡ã€åŒ»å­¦å½±åƒã€å«æ˜Ÿå›¾åƒ
- **éŸ³é¢‘æ•°æ®**ï¼šè¯­éŸ³è®°å½•ã€éŸ³ä¹ã€ç¯å¢ƒå£°éŸ³
- **è§†é¢‘æ•°æ®**ï¼šç›‘æ§å½•åƒã€ç›´æ’­æµã€æ•™å­¦è§†é¢‘

### å®æ—¶æ•°æ®æµ
- **æ¶ˆæ¯é˜Ÿåˆ—**ï¼šKafka, RabbitMQ, Apache Pulsar
- **APIæ¥å£**ï¼šREST API, GraphQL, WebSocket
- **IoTè®¾å¤‡**ï¼šä¼ æ„Ÿå™¨æ•°æ®ã€è®¾å¤‡çŠ¶æ€
- **ç¤¾äº¤åª’ä½“**ï¼šTwitter API, Facebook Graph API

## ğŸ› ï¸ æ•°æ®æå–æŠ€æœ¯

### SQLæ•°æ®æå–
```sql
-- åŸºç¡€æ•°æ®æŸ¥è¯¢
SELECT 
    customer_id,
    order_date,
    total_amount,
    product_category
FROM orders 
WHERE order_date >= '2024-01-01'
AND total_amount > 100;

-- å¤æ‚åˆ†ææŸ¥è¯¢
WITH monthly_sales AS (
    SELECT 
        DATE_TRUNC('month', order_date) as month,
        SUM(total_amount) as total_sales,
        COUNT(*) as order_count
    FROM orders
    GROUP BY DATE_TRUNC('month', order_date)
),
growth_analysis AS (
    SELECT 
        month,
        total_sales,
        LAG(total_sales) OVER (ORDER BY month) as prev_month_sales,
        (total_sales - LAG(total_sales) OVER (ORDER BY month)) / 
        LAG(total_sales) OVER (ORDER BY month) * 100 as growth_rate
    FROM monthly_sales
)
SELECT * FROM growth_analysis
WHERE growth_rate IS NOT NULL
ORDER BY month;
```

### Pythonæ•°æ®æå–
```python
import pandas as pd
import requests
import sqlalchemy
from sqlalchemy import create_engine
import pymongo
from bs4 import BeautifulSoup

# æ•°æ®åº“è¿æ¥
engine = create_engine('postgresql://user:password@localhost/database')

# SQLæŸ¥è¯¢
def extract_from_sql(query):
    """ä»SQLæ•°æ®åº“æå–æ•°æ®"""
    df = pd.read_sql(query, engine)
    return df

# APIæ•°æ®æå–
def extract_from_api(url, headers=None):
    """ä»REST APIæå–æ•°æ®"""
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")

# MongoDBæ•°æ®æå–
def extract_from_mongodb(collection_name, query=None):
    """ä»MongoDBæå–æ•°æ®"""
    client = pymongo.MongoClient('mongodb://localhost:27017/')
    db = client['database_name']
    collection = db[collection_name]
    
    cursor = collection.find(query or {})
    data = list(cursor)
    return pd.DataFrame(data)

# ç½‘é¡µçˆ¬å–
def extract_from_web(url):
    """ä»ç½‘é¡µæå–æ•°æ®"""
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # æå–è¡¨æ ¼æ•°æ®
    tables = soup.find_all('table')
    dfs = []
    for table in tables:
        df = pd.read_html(str(table))[0]
        dfs.append(df)
    
    return dfs
```

### æ–‡ä»¶æ•°æ®æå–
```python
import os
import json
import xml.etree.ElementTree as ET
from pathlib import Path

# CSVæ–‡ä»¶å¤„ç†
def process_csv_files(directory):
    """æ‰¹é‡å¤„ç†CSVæ–‡ä»¶"""
    csv_files = Path(directory).glob('*.csv')
    dfs = []
    
    for file_path in csv_files:
        try:
            df = pd.read_csv(file_path)
            df['source_file'] = file_path.name
            dfs.append(df)
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    return pd.concat(dfs, ignore_index=True)

# JSONæ–‡ä»¶å¤„ç†
def extract_from_json(file_path):
    """æå–JSONæ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å±•å¹³åµŒå¥—JSON
    def flatten_json(nested_json, separator='_'):
        out = {}
        def flatten(x, name=''):
            if type(x) is dict:
                for a in x:
                    flatten(x[a], name + a + separator)
            elif type(x) is list:
                i = 0
                for a in x:
                    flatten(a, name + str(i) + separator)
                    i += 1
            else:
                out[name[:-1]] = x
        flatten(nested_json)
        return out
    
    flattened = flatten_json(data)
    return pd.DataFrame([flattened])

# XMLæ–‡ä»¶å¤„ç†
def extract_from_xml(file_path):
    """æå–XMLæ•°æ®"""
    tree = ET.parse(file_path)
    root = tree.getroot()
    
    data = []
    for child in root:
        record = {}
        for subchild in child:
            record[subchild.tag] = subchild.text
        data.append(record)
    
    return pd.DataFrame(data)
```

## ğŸ”„ æ•°æ®è½¬æ¢ä¸æ¸…æ´—

### ETLæµæ°´çº¿
```python
import pandas as pd
from datetime import datetime
import re

class DataETLPipeline:
    def __init__(self):
        self.transformations = []
    
    def add_transformation(self, func):
        """æ·»åŠ è½¬æ¢å‡½æ•°"""
        self.transformations.append(func)
        return self
    
    def extract(self, source_config):
        """æ•°æ®æå–"""
        if source_config['type'] == 'sql':
            return extract_from_sql(source_config['query'])
        elif source_config['type'] == 'api':
            return pd.DataFrame(extract_from_api(source_config['url']))
        elif source_config['type'] == 'file':
            return pd.read_csv(source_config['path'])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {source_config['type']}")
    
    def transform(self, df):
        """æ•°æ®è½¬æ¢"""
        for transformation in self.transformations:
            df = transformation(df)
        return df
    
    def load(self, df, target_config):
        """æ•°æ®åŠ è½½"""
        if target_config['type'] == 'sql':
            df.to_sql(target_config['table'], engine, if_exists='replace')
        elif target_config['type'] == 'file':
            df.to_csv(target_config['path'], index=False)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›®æ ‡ç±»å‹: {target_config['type']}")

# å¸¸ç”¨è½¬æ¢å‡½æ•°
def clean_text_columns(df):
    """æ¸…æ´—æ–‡æœ¬åˆ—"""
    text_columns = df.select_dtypes(include=['object']).columns
    
    for col in text_columns:
        # å»é™¤å¤šä½™ç©ºæ ¼
        df[col] = df[col].astype(str).str.strip()
        # ç»Ÿä¸€å¤§å°å†™
        df[col] = df[col].str.lower()
        # å»é™¤ç‰¹æ®Šå­—ç¬¦
        df[col] = df[col].str.replace(r'[^\w\s]', '', regex=True)
    
    return df

def standardize_dates(df, date_columns):
    """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
    for col in date_columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')
    return df

def handle_missing_values(df, strategy='drop'):
    """å¤„ç†ç¼ºå¤±å€¼"""
    if strategy == 'drop':
        return df.dropna()
    elif strategy == 'fill_mean':
        numeric_columns = df.select_dtypes(include=['number']).columns
        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())
        return df
    elif strategy == 'fill_mode':
        categorical_columns = df.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            df[col] = df[col].fillna(df[col].mode()[0])
        return df
    else:
        return df

# ä½¿ç”¨ç¤ºä¾‹
pipeline = DataETLPipeline()
pipeline.add_transformation(clean_text_columns)\
        .add_transformation(lambda df: standardize_dates(df, ['order_date', 'delivery_date']))\
        .add_transformation(lambda df: handle_missing_values(df, 'fill_mean'))

# æ‰§è¡ŒETLæµç¨‹
source_config = {'type': 'sql', 'query': 'SELECT * FROM orders'}
target_config = {'type': 'file', 'path': 'cleaned_orders.csv'}

raw_data = pipeline.extract(source_config)
cleaned_data = pipeline.transform(raw_data)
pipeline.load(cleaned_data, target_config)
```

## ğŸ“Š é«˜çº§åˆ†ææŠ€æœ¯

### ç‰¹å¾å·¥ç¨‹
```python
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

def advanced_feature_engineering(df):
    """é«˜çº§ç‰¹å¾å·¥ç¨‹"""
    
    # æ—¶é—´ç‰¹å¾æå–
    if 'timestamp' in df.columns:
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['month'] = df['timestamp'].dt.month
        df['is_weekend'] = df['day_of_week'].isin([5, 6])
    
    # èšåˆç‰¹å¾
    if 'customer_id' in df.columns:
        customer_features = df.groupby('customer_id').agg({
            'order_amount': ['mean', 'std', 'count'],
            'product_category': lambda x: x.nunique()
        }).round(2)
        
        customer_features.columns = ['avg_order_amount', 'std_order_amount', 
                                   'total_orders', 'unique_categories']
        df = df.merge(customer_features, on='customer_id', how='left')
    
    # äº¤äº’ç‰¹å¾
    if 'price' in df.columns and 'quantity' in df.columns:
        df['total_value'] = df['price'] * df['quantity']
        df['price_per_unit'] = df['price'] / df['quantity']
    
    # æ–‡æœ¬ç‰¹å¾
    if 'description' in df.columns:
        tfidf = TfidfVectorizer(max_features=100, stop_words='english')
        tfidf_features = tfidf.fit_transform(df['description'])
        tfidf_df = pd.DataFrame(tfidf_features.toarray(), 
                               columns=[f'tfidf_{i}' for i in range(100)])
        df = pd.concat([df.reset_index(drop=True), tfidf_df], axis=1)
    
    return df
```

### å¼‚å¸¸æ£€æµ‹
```python
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

def detect_anomalies(df, method='isolation_forest'):
    """å¼‚å¸¸æ£€æµ‹"""
    
    # é€‰æ‹©æ•°å€¼åˆ—
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    X = df[numeric_columns].fillna(0)
    
    if method == 'isolation_forest':
        # å­¤ç«‹æ£®æ—
        detector = IsolationForest(contamination=0.1, random_state=42)
        anomalies = detector.fit_predict(X)
        
    elif method == 'dbscan':
        # DBSCANèšç±»
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        detector = DBSCAN(eps=0.5, min_samples=5)
        clusters = detector.fit_predict(X_scaled)
        anomalies = np.where(clusters == -1, -1, 1)
    
    # æ·»åŠ å¼‚å¸¸æ ‡è®°
    df['is_anomaly'] = anomalies == -1
    
    # å¯è§†åŒ–å¼‚å¸¸ç‚¹
    if X.shape[1] >= 2:
        plt.figure(figsize=(10, 6))
        normal_points = X[anomalies == 1]
        anomaly_points = X[anomalies == -1]
        
        plt.scatter(normal_points.iloc[:, 0], normal_points.iloc[:, 1], 
                   alpha=0.6, label='æ­£å¸¸ç‚¹')
        plt.scatter(anomaly_points.iloc[:, 0], anomaly_points.iloc[:, 1], 
                   alpha=0.8, color='red', label='å¼‚å¸¸ç‚¹')
        plt.xlabel(numeric_columns[0])
        plt.ylabel(numeric_columns[1])
        plt.legend()
        plt.title('å¼‚å¸¸æ£€æµ‹ç»“æœ')
        plt.show()
    
    return df
```

### å…³è”è§„åˆ™æŒ–æ˜
```python
from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd

def market_basket_analysis(df, transaction_col, item_col, min_support=0.01):
    """è´­ç‰©ç¯®åˆ†æ"""
    
    # åˆ›å»ºäº‹åŠ¡-å•†å“çŸ©é˜µ
    basket = df.groupby([transaction_col, item_col])['quantity'].sum().unstack().fillna(0)
    
    # è½¬æ¢ä¸ºå¸ƒå°”å€¼
    basket_bool = basket.applymap(lambda x: 1 if x > 0 else 0)
    
    # æŒ–æ˜é¢‘ç¹é¡¹é›†
    frequent_itemsets = apriori(basket_bool, min_support=min_support, use_colnames=True)
    
    # ç”Ÿæˆå…³è”è§„åˆ™
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.1)
    
    # æ’åºå¹¶å±•ç¤º
    rules_sorted = rules.sort_values('lift', ascending=False)
    
    print("Top 10 å…³è”è§„åˆ™:")
    print(rules_sorted[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))
    
    return rules_sorted

# ä½¿ç”¨ç¤ºä¾‹
# transaction_data = pd.read_csv('transactions.csv')
# rules = market_basket_analysis(transaction_data, 'transaction_id', 'product_name')
```

## ğŸ“ˆ å®æ—¶æ•°æ®åˆ†æ

### æµå¼æ•°æ®å¤„ç†
```python
import kafka
from kafka import KafkaConsumer
import json
import pandas as pd
from collections import deque
import threading
import time

class RealTimeAnalyzer:
    def __init__(self, kafka_topic, bootstrap_servers=['localhost:9092']):
        self.topic = kafka_topic
        self.servers = bootstrap_servers
        self.consumer = KafkaConsumer(
            kafka_topic,
            bootstrap_servers=bootstrap_servers,
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
        self.data_buffer = deque(maxlen=1000)  # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
        self.running = False
        
    def start_analysis(self):
        """å¼€å§‹å®æ—¶åˆ†æ"""
        self.running = True
        
        # å¯åŠ¨æ•°æ®æ¶ˆè´¹çº¿ç¨‹
        consumer_thread = threading.Thread(target=self._consume_data)
        consumer_thread.start()
        
        # å¯åŠ¨åˆ†æçº¿ç¨‹
        analysis_thread = threading.Thread(target=self._analyze_data)
        analysis_thread.start()
        
    def _consume_data(self):
        """æ¶ˆè´¹Kafkaæ•°æ®"""
        for message in self.consumer:
            if not self.running:
                break
            self.data_buffer.append(message.value)
    
    def _analyze_data(self):
        """åˆ†ææ•°æ®"""
        while self.running:
            if len(self.data_buffer) > 10:
                # è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame(list(self.data_buffer))
                
                # å®æ—¶ç»Ÿè®¡
                stats = {
                    'count': len(df),
                    'avg_value': df['value'].mean() if 'value' in df.columns else 0,
                    'max_value': df['value'].max() if 'value' in df.columns else 0,
                    'timestamp': time.time()
                }
                
                # å¼‚å¸¸æ£€æµ‹
                if 'value' in df.columns:
                    recent_values = df['value'].tail(50)
                    threshold = recent_values.mean() + 2 * recent_values.std()
                    current_value = df['value'].iloc[-1]
                    
                    if current_value > threshold:
                        print(f"å¼‚å¸¸å€¼æ£€æµ‹: {current_value} > {threshold}")
                
                print(f"å®æ—¶ç»Ÿè®¡: {stats}")
            
            time.sleep(5)  # æ¯5ç§’åˆ†æä¸€æ¬¡
    
    def stop_analysis(self):
        """åœæ­¢åˆ†æ"""
        self.running = False
        self.consumer.close()

# ä½¿ç”¨ç¤ºä¾‹
# analyzer = RealTimeAnalyzer('user_events')
# analyzer.start_analysis()
```

## ğŸ” æ•°æ®è´¨é‡è¯„ä¼°

### è‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥
```python
def assess_data_quality(df):
    """è¯„ä¼°æ•°æ®è´¨é‡"""
    
    quality_report = {}
    
    # å®Œæ•´æ€§æ£€æŸ¥
    missing_data = df.isnull().sum()
    quality_report['completeness'] = {
        'missing_values': missing_data.to_dict(),
        'missing_percentage': (missing_data / len(df) * 100).to_dict()
    }
    
    # å”¯ä¸€æ€§æ£€æŸ¥
    duplicate_count = df.duplicated().sum()
    quality_report['uniqueness'] = {
        'duplicate_rows': duplicate_count,
        'duplicate_percentage': duplicate_count / len(df) * 100
    }
    
    # ä¸€è‡´æ€§æ£€æŸ¥
    consistency_issues = []
    
    # æ£€æŸ¥æ•°å€¼åˆ—çš„å¼‚å¸¸å€¼
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
        if len(outliers) > 0:
            consistency_issues.append({
                'column': col,
                'issue': 'outliers',
                'count': len(outliers),
                'percentage': len(outliers) / len(df) * 100
            })
    
    quality_report['consistency'] = consistency_issues
    
    # æœ‰æ•ˆæ€§æ£€æŸ¥
    validity_issues = []
    
    # æ£€æŸ¥æ—¥æœŸåˆ—
    date_columns = df.select_dtypes(include=['datetime64']).columns
    for col in date_columns:
        future_dates = df[df[col] > pd.Timestamp.now()][col]
        if len(future_dates) > 0:
            validity_issues.append({
                'column': col,
                'issue': 'future_dates',
                'count': len(future_dates)
            })
    
    quality_report['validity'] = validity_issues
    
    return quality_report

# ç”Ÿæˆè´¨é‡æŠ¥å‘Š
quality_report = assess_data_quality(df)
print(json.dumps(quality_report, indent=2, default=str))
```

## ğŸ”— ç›¸å…³èµ„æº

### æå–å·¥å…·
- **Apache Airflow**ï¼šå·¥ä½œæµè°ƒåº¦å’Œç›‘æ§
- **Talend**ï¼šæ•°æ®é›†æˆå¹³å°
- **Apache NiFi**ï¼šæ•°æ®æµå¤„ç†
- **Pentaho**ï¼šå•†ä¸šæ™ºèƒ½å¥—ä»¶

### åˆ†ææ¡†æ¶
- **Apache Spark**ï¼šå¤§æ•°æ®å¤„ç†å¼•æ“
- **Dask**ï¼šå¹¶è¡Œè®¡ç®—åº“
- **Ray**ï¼šåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
- **Apache Kafka**ï¼šæµå¼æ•°æ®å¹³å°

## ğŸ”— å¯¼èˆªé“¾æ¥

- [è¿”å›ä¸»é¡µ](../index.html)
- [ä¸Šä¸€æ¨¡å—ï¼šæ•°æ®åˆ†æ](data-analysis.html)
- [ä¸‹ä¸€æ¨¡å—ï¼šæ•°æ®æ ‡æ³¨](data-annotation.html)
